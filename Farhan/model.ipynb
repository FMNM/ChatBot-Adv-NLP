{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import logging\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import google.generativeai as genai\n",
    "from google.oauth2 import service_account\n",
    "from pydantic import BaseModel, Field\n",
    "from rouge_score import rouge_scorer\n",
    "import sacrebleu\n",
    "from docx import Document as DocxDocument\n",
    "from tqdm import tqdm\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\farha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\farha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\farha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the service account's JSON file\n",
    "service_account_path = \"adv-nlp-uts-faa7595a22eb.json\"\n",
    "\n",
    "# Create credentials using the service account JSON file\n",
    "try:\n",
    "    credentials = service_account.Credentials.from_service_account_file(service_account_path, scopes=[\"https://www.googleapis.com/auth/generative-language\"])\n",
    "except FileNotFoundError:\n",
    "    logger.error(f\"Service account file not found at {service_account_path}.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error creating credentials from the service account file: {e}\")\n",
    "    raise\n",
    "\n",
    "# Configure the Gemini API client with the credentials\n",
    "genai.configure(credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text preprocessing function with lemmatization\n",
    "def preprocess_text(text):\n",
    "    # 1. Strip whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    # 2. Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # 3. Remove stopwords and apply lemmatization\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token.lower()) for token in tqdm(tokens, desc=\"Processing tokens\") if token.lower() not in stop_words]\n",
    "\n",
    "    # 4. Join the tokens back into a string\n",
    "    preprocessed_text = \" \".join(lemmatized_tokens)\n",
    "\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read .docx files from 'dataset/word_standards' folder\n",
    "def read_docx_files(folder_path):\n",
    "    documents = []\n",
    "    for filename in tqdm(os.listdir(folder_path), desc=\"Reading .docx files\"):\n",
    "        if filename.endswith(\".docx\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                docx_doc = DocxDocument(file_path)\n",
    "                full_text = []\n",
    "                for para in docx_doc.paragraphs:\n",
    "                    full_text.append(para.text)\n",
    "                text = \"\\n\".join(full_text)\n",
    "                # Create a LangChain Document object with text and metadata\n",
    "                langchain_doc = LangchainDocument(page_content=text, metadata={\"source\": filename})\n",
    "                documents.append(langchain_doc)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error reading {file_path}: {e}\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading .docx files:   0%|          | 0/91 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading .docx files: 100%|██████████| 91/91 [00:05<00:00, 15.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess documents from the folder\n",
    "folder_path = \"../data/word_standards\"\n",
    "documents = read_docx_files(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tokens: 100%|██████████| 6363/6363 [00:00<00:00, 348990.58it/s]\n",
      "Processing tokens: 100%|██████████| 1783/1783 [00:00<00:00, 395559.30it/s]\n",
      "Processing tokens: 100%|██████████| 2738/2738 [00:00<00:00, 271175.34it/s]\n",
      "Processing tokens: 100%|██████████| 1775/1775 [00:00<00:00, 253589.81it/s]\n",
      "Processing tokens: 100%|██████████| 3804/3804 [00:00<00:00, 341834.65it/s]\n",
      "Processing tokens: 100%|██████████| 2582/2582 [00:00<00:00, 363156.60it/s]\n",
      "Processing tokens: 100%|██████████| 2464/2464 [00:00<00:00, 404856.23it/s]\n",
      "Processing tokens: 100%|██████████| 5387/5387 [00:00<00:00, 359142.24it/s]\n",
      "Processing tokens: 100%|██████████| 6725/6725 [00:00<00:00, 417864.57it/s]\n",
      "Processing tokens: 100%|██████████| 6145/6145 [00:00<00:00, 407590.70it/s]\n",
      "Processing tokens: 100%|██████████| 2835/2835 [00:00<00:00, 350876.44it/s]\n",
      "Processing tokens: 100%|██████████| 2313/2313 [00:00<00:00, 385788.57it/s]\n",
      "Processing tokens: 100%|██████████| 2000/2000 [00:00<00:00, 329106.99it/s]\n",
      "Processing tokens: 100%|██████████| 4218/4218 [00:00<00:00, 417865.14it/s]\n",
      "Processing tokens: 100%|██████████| 2838/2838 [00:00<00:00, 400330.76it/s]\n",
      "Processing tokens: 100%|██████████| 2534/2534 [00:00<00:00, 362026.24it/s]\n",
      "Processing tokens: 100%|██████████| 2817/2817 [00:00<00:00, 397582.42it/s]\n",
      "Processing tokens: 100%|██████████| 2852/2852 [00:00<00:00, 475348.90it/s]\n",
      "Processing tokens: 100%|██████████| 4774/4774 [00:00<00:00, 425283.17it/s]\n",
      "Processing tokens: 100%|██████████| 3468/3468 [00:00<00:00, 361279.77it/s]\n",
      "Processing tokens: 100%|██████████| 2840/2840 [00:00<00:00, 405771.34it/s]\n",
      "Processing tokens: 100%|██████████| 5519/5519 [00:00<00:00, 419666.12it/s]\n",
      "Processing tokens: 100%|██████████| 2033/2033 [00:00<00:00, 338898.30it/s]\n",
      "Processing tokens: 100%|██████████| 6802/6802 [00:00<00:00, 468875.31it/s]\n",
      "Processing tokens: 100%|██████████| 2538/2538 [00:00<00:00, 416053.45it/s]\n",
      "Processing tokens: 100%|██████████| 1879/1879 [00:00<00:00, 375666.01it/s]\n",
      "Processing tokens: 100%|██████████| 5052/5052 [00:00<00:00, 421038.88it/s]\n",
      "Processing tokens: 100%|██████████| 2723/2723 [00:00<00:00, 340389.53it/s]\n",
      "Processing tokens: 100%|██████████| 13720/13720 [00:00<00:00, 412225.47it/s]\n",
      "Processing tokens: 100%|██████████| 15696/15696 [00:00<00:00, 400873.16it/s]\n",
      "Processing tokens: 100%|██████████| 3381/3381 [00:00<00:00, 372535.64it/s]\n",
      "Processing tokens: 100%|██████████| 1780/1780 [00:00<00:00, 254434.15it/s]\n",
      "Processing tokens: 100%|██████████| 1767/1767 [00:00<00:00, 346405.01it/s]\n",
      "Processing tokens: 100%|██████████| 1639/1639 [00:00<00:00, 410097.49it/s]\n",
      "Processing tokens: 100%|██████████| 3598/3598 [00:00<00:00, 359766.03it/s]\n",
      "Processing tokens: 100%|██████████| 3720/3720 [00:00<00:00, 336434.24it/s]\n",
      "Processing tokens: 100%|██████████| 5694/5694 [00:00<00:00, 291892.68it/s]\n",
      "Processing tokens: 100%|██████████| 2115/2115 [00:00<00:00, 296211.87it/s]\n",
      "Processing tokens: 100%|██████████| 3254/3254 [00:00<00:00, 406767.36it/s]\n",
      "Processing tokens: 100%|██████████| 1937/1937 [00:00<00:00, 387409.61it/s]\n",
      "Processing tokens: 100%|██████████| 3567/3567 [00:00<00:00, 356428.41it/s]\n",
      "Processing tokens: 100%|██████████| 3558/3558 [00:00<00:00, 391935.44it/s]\n",
      "Processing tokens: 100%|██████████| 3101/3101 [00:00<00:00, 387826.48it/s]\n",
      "Processing tokens: 100%|██████████| 2015/2015 [00:00<00:00, 403125.33it/s]\n",
      "Processing tokens: 100%|██████████| 2507/2507 [00:00<00:00, 418062.98it/s]\n",
      "Processing tokens: 100%|██████████| 4678/4678 [00:00<00:00, 389807.37it/s]\n",
      "Processing tokens: 100%|██████████| 1897/1897 [00:00<00:00, 344351.89it/s]\n",
      "Processing tokens: 100%|██████████| 2028/2028 [00:00<00:00, 405803.56it/s]\n",
      "Processing tokens: 100%|██████████| 2171/2171 [00:00<00:00, 434563.04it/s]\n",
      "Processing tokens: 100%|██████████| 5932/5932 [00:00<00:00, 393655.64it/s]\n",
      "Processing tokens: 100%|██████████| 2311/2311 [00:00<00:00, 462431.97it/s]\n",
      "Processing tokens: 100%|██████████| 5351/5351 [00:00<00:00, 406758.63it/s]\n",
      "Processing tokens: 100%|██████████| 1994/1994 [00:00<00:00, 390486.61it/s]\n",
      "Processing tokens: 100%|██████████| 3235/3235 [00:00<00:00, 359499.07it/s]\n",
      "Processing tokens: 100%|██████████| 3697/3697 [00:00<00:00, 308136.28it/s]\n",
      "Processing tokens: 100%|██████████| 1876/1876 [00:00<00:00, 375459.96it/s]\n",
      "Processing tokens: 100%|██████████| 2314/2314 [00:00<00:00, 385740.61it/s]\n",
      "Processing tokens: 100%|██████████| 3351/3351 [00:00<00:00, 414458.38it/s]\n",
      "Processing tokens: 100%|██████████| 3267/3267 [00:00<00:00, 363074.41it/s]\n",
      "Processing tokens: 100%|██████████| 2274/2274 [00:00<00:00, 455201.99it/s]\n",
      "Processing tokens: 100%|██████████| 4193/4193 [00:00<00:00, 378046.36it/s]\n",
      "Processing tokens: 100%|██████████| 24649/24649 [00:00<00:00, 456889.44it/s]\n",
      "Processing tokens: 100%|██████████| 1922/1922 [00:00<00:00, 315801.01it/s]\n",
      "Processing tokens: 100%|██████████| 21802/21802 [00:00<00:00, 413709.21it/s]\n",
      "Processing tokens: 100%|██████████| 3016/3016 [00:00<00:00, 430844.35it/s]\n",
      "Processing tokens: 100%|██████████| 3308/3308 [00:00<00:00, 466566.60it/s]\n",
      "Processing tokens: 100%|██████████| 11320/11320 [00:00<00:00, 449957.56it/s]\n",
      "Processing tokens: 100%|██████████| 5861/5861 [00:00<00:00, 447986.58it/s]\n",
      "Processing tokens: 100%|██████████| 8550/8550 [00:00<00:00, 415470.07it/s]\n",
      "Processing tokens: 100%|██████████| 8799/8799 [00:00<00:00, 334390.54it/s]\n",
      "Processing tokens: 100%|██████████| 13847/13847 [00:00<00:00, 416494.75it/s]\n",
      "Processing tokens: 100%|██████████| 5598/5598 [00:00<00:00, 396670.39it/s]\n",
      "Processing tokens: 100%|██████████| 12913/12913 [00:00<00:00, 414268.49it/s]\n",
      "Processing tokens: 100%|██████████| 4988/4988 [00:00<00:00, 412858.44it/s]\n",
      "Processing tokens: 100%|██████████| 5237/5237 [00:00<00:00, 416156.46it/s]\n",
      "Processing tokens: 100%|██████████| 3165/3165 [00:00<00:00, 452268.06it/s]\n",
      "Processing tokens: 100%|██████████| 3042/3042 [00:00<00:00, 377096.88it/s]\n",
      "Processing tokens: 100%|██████████| 4853/4853 [00:00<00:00, 373300.52it/s]\n",
      "Processing tokens: 100%|██████████| 2689/2689 [00:00<00:00, 244456.37it/s]\n",
      "Processing tokens: 100%|██████████| 9681/9681 [00:00<00:00, 400404.86it/s]\n",
      "Processing tokens: 100%|██████████| 2027/2027 [00:00<00:00, 331948.08it/s]\n",
      "Processing tokens: 100%|██████████| 5945/5945 [00:00<00:00, 440044.78it/s]\n",
      "Processing tokens: 100%|██████████| 4437/4437 [00:00<00:00, 394909.85it/s]\n",
      "Processing tokens: 100%|██████████| 8021/8021 [00:00<00:00, 420531.40it/s]\n",
      "Processing tokens: 100%|██████████| 2355/2355 [00:00<00:00, 392497.26it/s]\n",
      "Processing tokens: 100%|██████████| 2309/2309 [00:00<00:00, 329432.20it/s]\n",
      "Processing tokens: 100%|██████████| 4592/4592 [00:00<00:00, 377489.00it/s]\n",
      "Processing tokens: 100%|██████████| 3142/3142 [00:00<00:00, 392708.24it/s]\n",
      "Processing tokens: 100%|██████████| 7985/7985 [00:00<00:00, 385638.16it/s]\n",
      "Processing tokens: 100%|██████████| 12791/12791 [00:00<00:00, 422768.66it/s]\n",
      "Processing tokens: 100%|██████████| 17393/17393 [00:00<00:00, 444192.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the text in each document\n",
    "for doc in documents:\n",
    "    doc.page_content = preprocess_text(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the text splitter with overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2500,\n",
    "    chunk_overlap=500,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\", \"\\t\", \"\\r\\n\", \"\\r\", \"\\v\", \"\\f\", \"\\u0085\", \"\\u2028\", \"\\u2029\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Total number of documents after splitting: 1022\n"
     ]
    }
   ],
   "source": [
    "# Split the documents into chunks\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "logger.info(f\"Total number of documents after splitting: {len(split_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# Initialize the embeddings model\n",
    "embedding_model_name = \"all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "# Initialize FAISS vector store\n",
    "vector_store = FAISS.from_documents(split_docs, embeddings)\n",
    "\n",
    "# Save the vector store locally\n",
    "vector_store.save_local(\"faiss_index\")\n",
    "\n",
    "# To load the vector store from disk\n",
    "# vector_store = FAISS.load_local(\"faiss_index\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the Gemini LLM class\n",
    "class GeminiLLM(LLM, BaseModel):\n",
    "    model_name: str = Field(default=\"gemini-1.5-flash\")\n",
    "    temperature: float = Field(default=0.7)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self):\n",
    "        return \"gemini\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: list[str] = None) -> str:\n",
    "        try:\n",
    "            # Initialize the model\n",
    "            model = genai.GenerativeModel(model_name=self.model_name)\n",
    "\n",
    "            # Generate content using the Gemini API\n",
    "            response = model.generate_content(\n",
    "                prompt,\n",
    "                # temperature=self.temperature,\n",
    "                # max_output_tokens=512  # Adjust token limit as needed\n",
    "            )\n",
    "\n",
    "            # Extract generated text from the response\n",
    "            generated_text = response.text\n",
    "\n",
    "            # Handle stop tokens if provided\n",
    "            if stop:\n",
    "                for token in stop:\n",
    "                    generated_text = generated_text.split(token)[0]\n",
    "\n",
    "            return generated_text.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Gemini API error: {e}\")\n",
    "            return \"I'm sorry, but I couldn't process your request at this time.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Gemini LLM client\n",
    "llm = GeminiLLM(model_name=\"gemini-1.5-flash\", temperature=0.7)\n",
    "\n",
    "# Define a prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are an AI assistant with professional expertise in financial regulations and banking statistics, particularly knowledgeable about Australian APRA guidelines.\n",
    "\n",
    "Based on the provided context, please answer the following question in a clear, well detailed, and informative manner. Ensure your response directly addresses the query.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RetrievalQA chain with the custom prompt\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # You can experiment with 'refine' or 'map_reduce'\n",
    "    retriever=vector_store.as_retriever(search_kwargs={\"k\": 100}),\n",
    "    chain_type_kwargs={\"prompt\": prompt_template},\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle user queries\n",
    "def answer_query(query):\n",
    "    try:\n",
    "        response = qa_chain({\"query\": query})\n",
    "        answer = response[\"result\"]\n",
    "        source_docs = response[\"source_documents\"]\n",
    "        # print(\"Response:\")\n",
    "        print(answer)\n",
    "        # print(\"\\nRelevant Source Documents:\")\n",
    "        # for doc in source_docs:\n",
    "        #     print(f\"Source: {doc.metadata.get('source', 'Unknown Source')}\")\n",
    "        #     print(doc.page_content)\n",
    "        #     print(\"-\" * 80)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during query processing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RWA stands for **Risk-Weighted Asset**. \n",
      "\n",
      "It is a key concept in banking regulations, particularly within the context of capital adequacy requirements. RWA is calculated by multiplying an institution's assets by a risk weight that reflects the level of risk associated with those assets. This process is used to determine the amount of capital that an institution needs to hold to cover potential losses.\n",
      "\n",
      "**In the provided context, the RWA is calculated based on several factors:**\n",
      "\n",
      "* **Total IRRBB Capital Requirement:** This refers to the capital required to cover the interest rate risk in the banking book (IRRBB). \n",
      "* **Diversification Benefit Amount:** This represents the reduction in capital requirements due to diversification of assets. \n",
      "* **Factor of 12.5:** This is a constant factor applied to the total capital charge to arrive at the risk-weighted equivalent amount.\n",
      "\n",
      "**Here is a breakdown of how RWA is derived in the provided text:**\n",
      "\n",
      "1. The total IRRBB capital requirement is derived from a table, likely Table 1, as the sum of values in a specific column.\n",
      "2. The diversification benefit amount is subtracted from this total.\n",
      "3. The resulting value represents the total IRRBB capital requirement for the institution.\n",
      "4. The RWA equivalent amount is then calculated by multiplying this total IRRBB capital requirement by the factor of 12.5.\n",
      "\n",
      "**The purpose of RWA:**\n",
      "\n",
      "The RWA is used to determine the amount of capital a financial institution must hold to meet regulatory requirements and safeguard against potential losses. It plays a crucial role in assessing the financial stability of banks and other financial institutions, which are subject to APRA's guidelines.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is RWA?\"\n",
    "answer_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quality control for International Banking Statistics (IBS) Balance Sheet Items (ARF 731.4) involves a combination of internal review and external audit. \n",
      "\n",
      "**Internal Review:** \n",
      "\n",
      "* **Process Control:** Australian-owned banks, as the reporting entities, are required to have developed and implemented a process control system that ensures the completeness and reliability of the information provided in their ARF 731.4 submissions. \n",
      "* **Authorisation:** An authorized officer from the bank is responsible for submitting the information using the \"Direct APRA\" application method and digitally signing the relevant information using a digital certificate accepted by APRA.\n",
      "\n",
      "**External Audit:**\n",
      "\n",
      "* **Annual Review and Testing:** The information provided in ARF 731.4 must be subject to review and testing by the bank's external auditor on an annual basis, or more frequently as required. \n",
      "* **Scope and Nature of Review:** The external auditor must conduct a review to form an opinion on the accuracy and reliability of the information, adhering to the guidance provided by the Auditing and Assurance Standards Board's relevant standards. \n",
      "\n",
      "**Additionally, APRA may make minor alterations to the form or instructions to correct technical errors, inconsistencies, or anomalies.** However, any changes must be notified in writing to the bank requiring the report.\n",
      "\n",
      "**In summary, the quality control for IBS Balance Sheet Items focuses on ensuring the completeness, reliability, and accuracy of the data through internal processes and independent external audit.** This robust approach helps to maintain the integrity and usefulness of the data collected for prudential supervision purposes.\n"
     ]
    }
   ],
   "source": [
    "# Test the RAG system with a query\n",
    "query = \"What is the quality control on International Banking Statistics Balance Sheet Items?\"\n",
    "answer_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context you provided does not define \"Effective Maturity.\" The context focuses on the Australian Prudential Regulation Authority (APRA) guidelines for various aspects of financial institutions, including capital adequacy, liquidity risk, and remuneration. It extensively discusses the calculation of risk weights, available stable funding (ASF), required stable funding (RSF), and other related metrics. \n",
      "\n",
      "However, the definition of \"Effective Maturity\" is not explicitly mentioned in the text. To answer your question, I need more information about the context in which you are referring to \"Effective Maturity.\" \n",
      "\n",
      "Please provide:\n",
      "\n",
      "* **The specific document or section** where you encountered the term \"Effective Maturity.\"\n",
      "* **The context in which the term is used.** \n",
      "\n",
      "With this additional information, I can help you understand the key elements of the \"Effective Maturity\" definition and provide a clear and detailed answer.\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the key elements in the definition of Effective Maturity?\"\n",
    "answer_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluation functions (remain unchanged)\n",
    "# def evaluate_rouge(predicted, reference):\n",
    "#     scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rougeL\"], use_stemmer=True)\n",
    "#     return scorer.score(reference, predicted)\n",
    "\n",
    "\n",
    "# def evaluate_bleu(predicted, reference):\n",
    "#     bleu = sacrebleu.corpus_bleu([predicted], [[reference]])\n",
    "#     return bleu.score\n",
    "\n",
    "\n",
    "# def evaluate_f1(predicted, reference):\n",
    "#     predicted_tokens = nltk.word_tokenize(preprocess_text(predicted))\n",
    "#     reference_tokens = nltk.word_tokenize(preprocess_text(reference))\n",
    "#     common_tokens = set(predicted_tokens) & set(reference_tokens)\n",
    "\n",
    "#     precision = len(common_tokens) / len(predicted_tokens) if predicted_tokens else 0\n",
    "#     recall = len(common_tokens) / len(reference_tokens) if reference_tokens else 0\n",
    "\n",
    "#     if precision + recall == 0:\n",
    "#         return 0.0\n",
    "#     return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "# def run_evaluation(file_path):\n",
    "#     with open(file_path, \"r\") as f:\n",
    "#         data = json.load(f)\n",
    "\n",
    "#     rouge_scores, bleu_scores, f1_scores = [], [], []\n",
    "#     chat_history = []\n",
    "\n",
    "#     for entry in data:\n",
    "#         question = entry[\"question\"]\n",
    "#         reference_answer = entry[\"answer\"]\n",
    "#         predicted_answer = answer_query(question, chat_history)\n",
    "\n",
    "#         rouge = evaluate_rouge(predicted_answer, reference_answer)\n",
    "#         bleu = evaluate_bleu(predicted_answer, reference_answer)\n",
    "#         f1 = evaluate_f1(predicted_answer, reference_answer)\n",
    "\n",
    "#         rouge_scores.append(rouge[\"rougeL\"].fmeasure)\n",
    "#         bleu_scores.append(bleu)\n",
    "#         f1_scores.append(f1)\n",
    "\n",
    "#         print(f\"Q: {question}\")\n",
    "#         print(f\"Predicted: {predicted_answer}\")\n",
    "#         print(f\"Reference: {reference_answer}\")\n",
    "#         print(f\"ROUGE-L: {rouge}\")\n",
    "#         print(f\"BLEU: {bleu}\")\n",
    "#         print(f\"F1: {f1}\")\n",
    "#         print(\"-\" * 80)\n",
    "\n",
    "#     print(\"\\n=== Evaluation Summary ===\")\n",
    "#     print(f\"Average ROUGE-L: {sum(rouge_scores) / len(rouge_scores):.4f}\")\n",
    "#     print(f\"Average BLEU: {sum(bleu_scores) / len(bleu_scores):.2f}\")\n",
    "#     print(f\"Average F1: {sum(f1_scores) / len(f1_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_evaluation(\"questions_answers.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
