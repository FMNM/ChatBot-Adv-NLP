{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import logging\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import google.generativeai as genai\n",
    "from google.oauth2 import service_account\n",
    "from pydantic import BaseModel, Field\n",
    "from rouge_score import rouge_scorer\n",
    "from collections import Counter\n",
    "import re\n",
    "import sacrebleu\n",
    "from collections import Counter\n",
    "from docx import Document as DocxDocument\n",
    "from tqdm import tqdm\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "from bert_score import score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\farha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\farha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\farha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text preprocessing function with lemmatization\n",
    "def preprocess_text(text):\n",
    "    # 1. Strip whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    # 2. Remove special characters and numbers (optional, depending on requirements)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "\n",
    "    # 3. Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # 4. Remove stopwords and apply lemmatization\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token.lower()) for token in tqdm(tokens, desc=\"Processing tokens\") if token.lower() not in stop_words]\n",
    "\n",
    "    # 5. Join the tokens back into a string\n",
    "    preprocessed_text = \" \".join(lemmatized_tokens)\n",
    "\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read .docx files from 'dataset/word_standards' folder\n",
    "def read_docx_files(folder_path):\n",
    "    documents = []\n",
    "    for filename in tqdm(os.listdir(folder_path), desc=\"Reading .docx files\"):\n",
    "        if filename.endswith(\".docx\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                docx_doc = DocxDocument(file_path)\n",
    "                full_text = []\n",
    "                for para in docx_doc.paragraphs:\n",
    "                    full_text.append(para.text)\n",
    "                text = \"\\n\".join(full_text)\n",
    "                # Create a LangChain Document object with text and metadata\n",
    "                langchain_doc = LangchainDocument(page_content=text, metadata={\"source\": filename})\n",
    "                documents.append(langchain_doc)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error reading {file_path}: {e}\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading .docx files: 100%|██████████| 91/91 [00:04<00:00, 18.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess documents from the folder\n",
    "folder_path = \"../data/word_standards\"\n",
    "documents = read_docx_files(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tokens: 100%|██████████| 5660/5660 [00:00<00:00, 435416.17it/s]\n",
      "Processing tokens: 100%|██████████| 1495/1495 [00:00<00:00, 291813.31it/s]\n",
      "Processing tokens: 100%|██████████| 2294/2294 [00:00<00:00, 458330.56it/s]\n",
      "Processing tokens: 100%|██████████| 1500/1500 [00:00<00:00, 375161.36it/s]\n",
      "Processing tokens: 100%|██████████| 3281/3281 [00:00<00:00, 468732.29it/s]\n",
      "Processing tokens: 100%|██████████| 2210/2210 [00:00<00:00, 442010.96it/s]\n",
      "Processing tokens: 100%|██████████| 2094/2094 [00:00<00:00, 418790.41it/s]\n",
      "Processing tokens: 100%|██████████| 4567/4567 [00:00<00:00, 415166.92it/s]\n",
      "Processing tokens: 100%|██████████| 5778/5778 [00:00<00:00, 444510.06it/s]\n",
      "Processing tokens: 100%|██████████| 5287/5287 [00:00<00:00, 440676.56it/s]\n",
      "Processing tokens: 100%|██████████| 2395/2395 [00:00<00:00, 399163.88it/s]\n",
      "Processing tokens: 100%|██████████| 1981/1981 [00:00<00:00, 385994.44it/s]\n",
      "Processing tokens: 100%|██████████| 1686/1686 [00:00<00:00, 373743.28it/s]\n",
      "Processing tokens: 100%|██████████| 3591/3591 [00:00<00:00, 448894.16it/s]\n",
      "Processing tokens: 100%|██████████| 2411/2411 [00:00<00:00, 482349.96it/s]\n",
      "Processing tokens: 100%|██████████| 2151/2151 [00:00<00:00, 430518.61it/s]\n",
      "Processing tokens: 100%|██████████| 2390/2390 [00:00<00:00, 477715.71it/s]\n",
      "Processing tokens: 100%|██████████| 2426/2426 [00:00<00:00, 404266.25it/s]\n",
      "Processing tokens: 100%|██████████| 4136/4136 [00:00<00:00, 452976.51it/s]\n",
      "Processing tokens: 100%|██████████| 2958/2958 [00:00<00:00, 454409.82it/s]\n",
      "Processing tokens: 100%|██████████| 2414/2414 [00:00<00:00, 402234.62it/s]\n",
      "Processing tokens: 100%|██████████| 4691/4691 [00:00<00:00, 466166.28it/s]\n",
      "Processing tokens: 100%|██████████| 1732/1732 [00:00<00:00, 433289.67it/s]\n",
      "Processing tokens: 100%|██████████| 5873/5873 [00:00<00:00, 451727.41it/s]\n",
      "Processing tokens: 100%|██████████| 2143/2143 [00:00<00:00, 535214.57it/s]\n",
      "Processing tokens: 100%|██████████| 1625/1625 [00:00<00:00, 406303.67it/s]\n",
      "Processing tokens: 100%|██████████| 4335/4335 [00:00<00:00, 433603.79it/s]\n",
      "Processing tokens: 100%|██████████| 2313/2313 [00:00<00:00, 385589.23it/s]\n",
      "Processing tokens: 100%|██████████| 11415/11415 [00:00<00:00, 386739.64it/s]\n",
      "Processing tokens: 100%|██████████| 13090/13090 [00:00<00:00, 436337.22it/s]\n",
      "Processing tokens: 100%|██████████| 2792/2792 [00:00<00:00, 398940.41it/s]\n",
      "Processing tokens: 100%|██████████| 1475/1475 [00:00<00:00, 245919.56it/s]\n",
      "Processing tokens: 100%|██████████| 1469/1469 [00:00<00:00, 325743.20it/s]\n",
      "Processing tokens: 100%|██████████| 1390/1390 [00:00<00:00, 464030.77it/s]\n",
      "Processing tokens: 100%|██████████| 3035/3035 [00:00<00:00, 429491.97it/s]\n",
      "Processing tokens: 100%|██████████| 3170/3170 [00:00<00:00, 452859.12it/s]\n",
      "Processing tokens: 100%|██████████| 4989/4989 [00:00<00:00, 453597.99it/s]\n",
      "Processing tokens: 100%|██████████| 1801/1801 [00:00<00:00, 450443.74it/s]\n",
      "Processing tokens: 100%|██████████| 2859/2859 [00:00<00:00, 399437.56it/s]\n",
      "Processing tokens: 100%|██████████| 1616/1616 [00:00<00:00, 461276.39it/s]\n",
      "Processing tokens: 100%|██████████| 3074/3074 [00:00<00:00, 439159.73it/s]\n",
      "Processing tokens: 100%|██████████| 3097/3097 [00:00<00:00, 442430.50it/s]\n",
      "Processing tokens: 100%|██████████| 2606/2606 [00:00<00:00, 434330.30it/s]\n",
      "Processing tokens: 100%|██████████| 1730/1730 [00:00<00:00, 432402.47it/s]\n",
      "Processing tokens: 100%|██████████| 2163/2163 [00:00<00:00, 432858.42it/s]\n",
      "Processing tokens: 100%|██████████| 4040/4040 [00:00<00:00, 448933.32it/s]\n",
      "Processing tokens: 100%|██████████| 1623/1623 [00:00<00:00, 541814.34it/s]\n",
      "Processing tokens: 100%|██████████| 1728/1728 [00:00<00:00, 383033.36it/s]\n",
      "Processing tokens: 100%|██████████| 1868/1868 [00:00<00:00, 373769.67it/s]\n",
      "Processing tokens: 100%|██████████| 4897/4897 [00:00<00:00, 376740.34it/s]\n",
      "Processing tokens: 100%|██████████| 1952/1952 [00:00<00:00, 488413.85it/s]\n",
      "Processing tokens: 100%|██████████| 4528/4528 [00:00<00:00, 411621.59it/s]\n",
      "Processing tokens: 100%|██████████| 1692/1692 [00:00<00:00, 423560.87it/s]\n",
      "Processing tokens: 100%|██████████| 2763/2763 [00:00<00:00, 394917.77it/s]\n",
      "Processing tokens: 100%|██████████| 3139/3139 [00:00<00:00, 448415.25it/s]\n",
      "Processing tokens: 100%|██████████| 1585/1585 [00:00<00:00, 396586.04it/s]\n",
      "Processing tokens: 100%|██████████| 1921/1921 [00:00<00:00, 320177.15it/s]\n",
      "Processing tokens: 100%|██████████| 2893/2893 [00:00<00:00, 413315.67it/s]\n",
      "Processing tokens: 100%|██████████| 2848/2848 [00:00<00:00, 406900.49it/s]\n",
      "Processing tokens: 100%|██████████| 1925/1925 [00:00<00:00, 384734.36it/s]\n",
      "Processing tokens: 100%|██████████| 3549/3549 [00:00<00:00, 394309.68it/s]\n",
      "Processing tokens: 100%|██████████| 21832/21832 [00:00<00:00, 472039.74it/s]\n",
      "Processing tokens: 100%|██████████| 1566/1566 [00:00<00:00, 522619.36it/s]\n",
      "Processing tokens: 100%|██████████| 19434/19434 [00:00<00:00, 451853.45it/s]\n",
      "Processing tokens: 100%|██████████| 2652/2652 [00:00<00:00, 378910.42it/s]\n",
      "Processing tokens: 100%|██████████| 2863/2863 [00:00<00:00, 409085.38it/s]\n",
      "Processing tokens: 100%|██████████| 10037/10037 [00:00<00:00, 456275.18it/s]\n",
      "Processing tokens: 100%|██████████| 5124/5124 [00:00<00:00, 445191.38it/s]\n",
      "Processing tokens: 100%|██████████| 7576/7576 [00:00<00:00, 420880.37it/s]\n",
      "Processing tokens: 100%|██████████| 7670/7670 [00:00<00:00, 479245.49it/s]\n",
      "Processing tokens: 100%|██████████| 12289/12289 [00:00<00:00, 455052.55it/s]\n",
      "Processing tokens: 100%|██████████| 4759/4759 [00:00<00:00, 432761.53it/s]\n",
      "Processing tokens: 100%|██████████| 10748/10748 [00:00<00:00, 467251.03it/s]\n",
      "Processing tokens: 100%|██████████| 4217/4217 [00:00<00:00, 421790.91it/s]\n",
      "Processing tokens: 100%|██████████| 4355/4355 [00:00<00:00, 435490.03it/s]\n",
      "Processing tokens: 100%|██████████| 2685/2685 [00:00<00:00, 447550.22it/s]\n",
      "Processing tokens: 100%|██████████| 2580/2580 [00:00<00:00, 430099.54it/s]\n",
      "Processing tokens: 100%|██████████| 4090/4090 [00:00<00:00, 454260.76it/s]\n",
      "Processing tokens: 100%|██████████| 2285/2285 [00:00<00:00, 457011.33it/s]\n",
      "Processing tokens: 100%|██████████| 8357/8357 [00:00<00:00, 398003.82it/s]\n",
      "Processing tokens: 100%|██████████| 1753/1753 [00:00<00:00, 304342.68it/s]\n",
      "Processing tokens: 100%|██████████| 5253/5253 [00:00<00:00, 437799.13it/s]\n",
      "Processing tokens: 100%|██████████| 3824/3824 [00:00<00:00, 424807.14it/s]\n",
      "Processing tokens: 100%|██████████| 6877/6877 [00:00<00:00, 415970.53it/s]\n",
      "Processing tokens: 100%|██████████| 2009/2009 [00:00<00:00, 502346.29it/s]\n",
      "Processing tokens: 100%|██████████| 1956/1956 [00:00<00:00, 325984.77it/s]\n",
      "Processing tokens: 100%|██████████| 4008/4008 [00:00<00:00, 400857.73it/s]\n",
      "Processing tokens: 100%|██████████| 2729/2729 [00:00<00:00, 389872.12it/s]\n",
      "Processing tokens: 100%|██████████| 6939/6939 [00:00<00:00, 462670.30it/s]\n",
      "Processing tokens: 100%|██████████| 11245/11245 [00:00<00:00, 488892.73it/s]\n",
      "Processing tokens: 100%|██████████| 15170/15170 [00:00<00:00, 459633.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the text in each document\n",
    "for doc in documents:\n",
    "    doc.page_content = preprocess_text(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving to Vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the text splitter with overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=3000,\n",
    "    chunk_overlap=600,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\", \"\\t\", \"\\r\\n\", \"\\r\", \"\\v\", \"\\f\", \"\\u0085\", \"\\u2028\", \"\\u2029\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Total number of documents after splitting: 792\n"
     ]
    }
   ],
   "source": [
    "# Split the documents into chunks\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "logger.info(f\"Total number of documents after splitting: {len(split_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\farha\\AppData\\Local\\Temp\\ipykernel_9580\\2581478539.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "c:\\Users\\farha\\miniconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
      "INFO:faiss.loader:Could not load library with AVX2 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\n",
      "INFO:faiss.loader:Loading faiss.\n",
      "INFO:faiss.loader:Successfully loaded faiss.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the embeddings model\n",
    "embedding_model_name = \"all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "# Initialize FAISS vector store\n",
    "vector_store = FAISS.from_documents(split_docs, embeddings)\n",
    "\n",
    "# Save the vector store locally\n",
    "vector_store.save_local(\"faiss_index\")\n",
    "\n",
    "# # To load the vector store from disk\n",
    "# vector_store = FAISS.load_local(\"faiss_index\", embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini API configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the service account's JSON file\n",
    "service_account_path = \"adv-nlp-uts-faa7595a22eb.json\"\n",
    "\n",
    "# Create credentials using the service account JSON file\n",
    "try:\n",
    "    credentials = service_account.Credentials.from_service_account_file(service_account_path, scopes=[\"https://www.googleapis.com/auth/generative-language\"])\n",
    "except FileNotFoundError:\n",
    "    logger.error(f\"Service account file not found at {service_account_path}.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error creating credentials from the service account file: {e}\")\n",
    "    raise\n",
    "\n",
    "# Configure the Gemini API client with the credentials\n",
    "genai.configure(credentials=credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring Gemini-1.5-Flash as serving LLM client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the Gemini LLM class\n",
    "class GeminiLLM(LLM, BaseModel):\n",
    "    model_name: str = Field(default=\"gemini-1.5-flash\")\n",
    "    temperature: float = Field(default=0.7)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self):\n",
    "        return \"gemini\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: list[str] = None) -> str:\n",
    "        try:\n",
    "            # Initialize the model\n",
    "            model = genai.GenerativeModel(model_name=self.model_name)\n",
    "\n",
    "            # Generate content using the Gemini API\n",
    "            response = model.generate_content(\n",
    "                prompt,\n",
    "                # temperature=self.temperature,\n",
    "                # max_output_tokens=512  # Adjust token limit as needed\n",
    "            )\n",
    "\n",
    "            # Extract generated text from the response\n",
    "            generated_text = response.text\n",
    "\n",
    "            # Handle stop tokens if provided\n",
    "            if stop:\n",
    "                for token in stop:\n",
    "                    generated_text = generated_text.split(token)[0]\n",
    "\n",
    "            return generated_text.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Gemini API error: {e}\")\n",
    "            return \"I'm sorry, but I couldn't process your request at this time.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Gemini LLM client\n",
    "llm = GeminiLLM(model_name=\"gemini-1.5-flash\", temperature=0.7)\n",
    "\n",
    "# Define a prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are an AI assistant with professional expertise in financial regulations and banking statistics, particularly knowledgeable about Australian APRA guidelines.\n",
    "Based on the provided context, please answer the following question in a clear, well detailed, and informative manner. \n",
    "Ensure your response directly addresses the query and do not generate information that is not supported by the provided content.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the RAG agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RetrievalQA chain with the custom prompt\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # You can experiment with 'refine' or 'map_reduce'\n",
    "    retriever=vector_store.as_retriever(search_kwargs={\"k\": 100}),\n",
    "    chain_type_kwargs={\"prompt\": prompt_template},\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle user queries\n",
    "def answer_query(query):\n",
    "    try:\n",
    "        response = qa_chain({\"query\": query})\n",
    "        answer = response[\"result\"]\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during query processing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to handle user queries\n",
    "# def answer_query(query):\n",
    "#     try:\n",
    "#         response = qa_chain({\"query\": query})\n",
    "#         answer = response[\"result\"]\n",
    "#         # source_docs = response[\"source_documents\"]\n",
    "#         # print(\"Response:\")\n",
    "#         # print(answer)\n",
    "#         # print(\"\\nRelevant Source Documents:\")\n",
    "#         # for doc in source_docs:\n",
    "#         #     print(f\"Source: {doc.metadata.get('source', 'Unknown Source')}\")\n",
    "#         #     print(doc.page_content)\n",
    "#         #     print(\"-\" * 80)\n",
    "#         return answer\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Error during query processing: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the RAG agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\farha\\AppData\\Local\\Temp\\ipykernel_9580\\3558265379.py:4: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain({\"query\": query})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'RWA stands for **risk-weighted asset**. It is the total value of an asset multiplied by the risk weight assigned to that asset class, as outlined in the prudential standard APS Capital Adequacy Standardised Approach Credit Risk.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the RAG system with a query\n",
    "query = \"What is RWA?\"\n",
    "# query = \"What is the quality control on International Banking Statistics Balance Sheet Items?\"\n",
    "# query = \"What are the key elements in the definition of Effective Maturity?\"\n",
    "answer_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of RAG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"../data/question-answers.csv\"\n",
    "try:\n",
    "    # Load the CSV file with a specified encoding\n",
    "    data = pd.read_csv(csv_path, encoding=\"utf-8\")\n",
    "except UnicodeDecodeError:\n",
    "    # If utf-8 fails, try a different encoding\n",
    "    data = pd.read_csv(csv_path, encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_rouge(predicted, reference):\n",
    "#     if predicted is None:\n",
    "#         predicted = \"\"\n",
    "#     if reference is None:\n",
    "#         reference = \"\"\n",
    "#     scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rougeL\"], use_stemmer=True)\n",
    "#     return scorer.score(reference, predicted)\n",
    "\n",
    "\n",
    "# def evaluate_bleu(predicted, reference):\n",
    "#     if predicted is None:\n",
    "#         predicted = \"\"\n",
    "#     if reference is None:\n",
    "#         reference = \"\"\n",
    "#     bleu = sacrebleu.corpus_bleu([predicted], [[reference]])\n",
    "#     return bleu.score\n",
    "\n",
    "\n",
    "# def evaluate_f1(predicted, reference):\n",
    "#     # Handling None values\n",
    "#     if predicted is None:\n",
    "#         predicted = \"\"\n",
    "#     if reference is None:\n",
    "#         reference = \"\"\n",
    "\n",
    "#     # Preprocess both predicted and reference\n",
    "#     predicted_tokens = nltk.word_tokenize(preprocess_text(predicted))\n",
    "#     reference_tokens = nltk.word_tokenize(preprocess_text(reference))\n",
    "\n",
    "#     # Use Counter to keep token frequency for precision and recall calculations\n",
    "#     predicted_counts = Counter(predicted_tokens)\n",
    "#     reference_counts = Counter(reference_tokens)\n",
    "\n",
    "#     # Calculate true positives (common tokens)\n",
    "#     common_tokens = sum((predicted_counts & reference_counts).values())\n",
    "\n",
    "#     # Calculate precision and recall\n",
    "#     precision = common_tokens / sum(predicted_counts.values()) if predicted_counts else 0\n",
    "#     recall = common_tokens / sum(reference_counts.values()) if reference_counts else 0\n",
    "\n",
    "#     # Calculate F1 score\n",
    "#     if precision + recall == 0:\n",
    "#         return 0.0\n",
    "#     return 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "def evaluate_rouge(predicted, reference):\n",
    "    if predicted is None:\n",
    "        predicted = \"\"\n",
    "    if reference is None:\n",
    "        reference = \"\"\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rougeL\"], use_stemmer=True)\n",
    "    return scorer.score(reference, predicted)\n",
    "\n",
    "\n",
    "def evaluate_bleu(predicted, reference):\n",
    "    if predicted is None:\n",
    "        predicted = \"\"\n",
    "    if reference is None:\n",
    "        reference = \"\"\n",
    "    bleu = sacrebleu.corpus_bleu([predicted], [[reference]])\n",
    "    return bleu.score\n",
    "\n",
    "\n",
    "def evaluate_f1(predicted, reference):\n",
    "    if predicted is None:\n",
    "        predicted = \"\"\n",
    "    if reference is None:\n",
    "        reference = \"\"\n",
    "    predicted_tokens = nltk.word_tokenize(preprocess_text(predicted))\n",
    "    reference_tokens = nltk.word_tokenize(preprocess_text(reference))\n",
    "    predicted_counts = Counter(predicted_tokens)\n",
    "    reference_counts = Counter(reference_tokens)\n",
    "\n",
    "    common_tokens = sum((predicted_counts & reference_counts).values())\n",
    "\n",
    "    precision = common_tokens / sum(predicted_counts.values()) if predicted_counts else 0\n",
    "    recall = common_tokens / sum(reference_counts.values()) if reference_counts else 0\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "def evaluate_bert_score(predicted, reference):\n",
    "    if predicted is None:\n",
    "        predicted = \"\"\n",
    "    if reference is None:\n",
    "        reference = \"\"\n",
    "    P, R, F1 = score([predicted], [reference], lang=\"en\", verbose=False)\n",
    "    return F1.mean().item()\n",
    "\n",
    "\n",
    "# Initialize SentenceTransformer for embedding similarity\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "def evaluate_embedding_similarity(predicted, reference):\n",
    "    if predicted is None:\n",
    "        predicted = \"\"\n",
    "    if reference is None:\n",
    "        reference = \"\"\n",
    "    predicted_embedding = sentence_model.encode(predicted)\n",
    "    reference_embedding = sentence_model.encode(reference)\n",
    "    similarity = cosine_similarity([predicted_embedding], [reference_embedding])[0][0]\n",
    "    return similarity\n",
    "\n",
    "\n",
    "def evaluate_hybrid(predicted, reference):\n",
    "    rouge = evaluate_rouge(predicted, reference)\n",
    "    rouge_score = rouge[\"rougeL\"].fmeasure\n",
    "\n",
    "    bleu_score = evaluate_bleu(predicted, reference)\n",
    "    f1_score = evaluate_f1(predicted, reference)\n",
    "    bert_score = evaluate_bert_score(predicted, reference)\n",
    "    embedding_similarity = evaluate_embedding_similarity(predicted, reference)\n",
    "\n",
    "    # Define a weighted hybrid score\n",
    "    hybrid_score = (0.2 * rouge_score) + (0.2 * bleu_score) + (0.2 * f1_score) + (0.2 * bert_score) + (0.2 * embedding_similarity)\n",
    "    return hybrid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column for predicted answers\n",
    "data[\"RAG 1\"] = \"\"\n",
    "\n",
    "# Initialize list to hold the predicted answers\n",
    "predicted_answers = []\n",
    "\n",
    "# Initialize lists to hold metric scores\n",
    "rouge_scores, bleu_scores, f1_scores, bert_scores, embedding_similarities, hybrid_scores = [], [], [], [], [], []\n",
    "\n",
    "# Initialize min and max values for each metric\n",
    "rouge_min, rouge_max = float(\"inf\"), float(\"-inf\")\n",
    "bleu_min, bleu_max = float(\"inf\"), float(\"-inf\")\n",
    "f1_min, f1_max = float(\"inf\"), float(\"-inf\")\n",
    "bert_min, bert_max = float(\"inf\"), float(\"-inf\")\n",
    "embedding_min, embedding_max = float(\"inf\"), float(\"-inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:   0%|          | 0/80 [00:00<?, ?it/s]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 109/109 [00:00<00:00, 109033.90it/s]\n",
      "Processing tokens: 100%|██████████| 57/57 [00:00<00:00, 57017.73it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972285e3aee54db0994e20a696a86f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529eac7cc14e418198754320d62a4838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:   1%|▏         | 1/80 [00:04<05:50,  4.44s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 52/52 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 156/156 [00:00<00:00, 156048.52it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b626204fb533451aaf4a0bea2b85ef45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "088dcb8309b948198b96c37e02380895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:   2%|▎         | 2/80 [00:08<05:17,  4.07s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 87/87 [00:00<00:00, 86944.11it/s]\n",
      "Processing tokens: 100%|██████████| 43/43 [00:00<00:00, 42982.62it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d410699a7a4e8da706338f91b3ee49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb34dc9e2f84ec99309664862411865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:   4%|▍         | 3/80 [00:12<05:18,  4.13s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 23/23 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 45/45 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b85f4c28db48c3afcc62f08e7a57b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d3c48d59b4400ba665c41b633e7db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:   5%|▌         | 4/80 [00:15<04:46,  3.77s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 80/80 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 119/119 [00:00<00:00, 118980.26it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "627455094405414997ac76dc5bd26c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76e30fc3c70446b8a6868eef4f9bad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:   6%|▋         | 5/80 [00:19<04:41,  3.75s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 398/398 [00:00<00:00, 199156.88it/s]\n",
      "Processing tokens: 100%|██████████| 47/47 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53806d33545546a5a515c53fe2721d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d9cc92bc0804e989ef938d7a7485424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:   8%|▊         | 6/80 [00:26<06:03,  4.91s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 262/262 [00:00<00:00, 261894.10it/s]\n",
      "Processing tokens: 100%|██████████| 214/214 [00:00<00:00, 213862.53it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e3dcc224e9940faa893c476bccf5e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d7e8de0f744f0f95095de5d219333a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:   9%|▉         | 7/80 [00:32<06:18,  5.18s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 299/299 [00:00<00:00, 298665.61it/s]\n",
      "Processing tokens: 100%|██████████| 276/276 [00:00<00:00, 275036.33it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b61b1228e4e46d7b46f4e164ccdd06b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3097608b611b407c88caca1c32d96bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  10%|█         | 8/80 [00:37<06:23,  5.33s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 311/311 [00:00<00:00, 311468.13it/s]\n",
      "Processing tokens: 100%|██████████| 27/27 [00:00<00:00, 27034.19it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ae335bbc8641b28ee603cb87081de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db1f66a0b11483f8109141fd0bc9b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  11%|█▏        | 9/80 [00:43<06:27,  5.46s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 24/24 [00:00<00:00, 24059.11it/s]\n",
      "Processing tokens: 100%|██████████| 80/80 [00:00<00:00, 79872.49it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2325b5c1f7840998416574ee1bc9adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d181732e2743498eee2d124c465e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  12%|█▎        | 10/80 [00:47<05:40,  4.87s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 244/244 [00:00<00:00, 243959.52it/s]\n",
      "Processing tokens: 100%|██████████| 327/327 [00:00<00:00, 163492.36it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf88bba84f64c24bef5b015a93ae5a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09245b48d7848429bf7657ab0885cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  14%|█▍        | 11/80 [00:52<05:43,  4.97s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 560/560 [00:00<00:00, 280187.31it/s]\n",
      "Processing tokens: 100%|██████████| 42/42 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2470f1dc5f21473f8207718a51130920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a006d30fc14b4f6ea77de0bb984dff1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  15%|█▌        | 12/80 [01:01<06:56,  6.12s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 404/404 [00:00<00:00, 403356.06it/s]\n",
      "Processing tokens: 100%|██████████| 426/426 [00:00<00:00, 425929.32it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3788d24f0dcc43b8ba6b7092f9e50c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d8fee8aa9e4f04b4d912c9f847aeef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  16%|█▋        | 13/80 [01:07<06:58,  6.24s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 19/19 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 43/43 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c9f2ed5404f4ef0872dedfd8c07326e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c919be31624493fb2dab4b50f9acd4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  18%|█▊        | 14/80 [01:10<05:51,  5.33s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 15/15 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 22/22 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "017f0501c18047518cc1ab2dd1d41f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4011872d70045758d2e2805b065f1b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  19%|█▉        | 15/80 [01:14<05:04,  4.69s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 212/212 [00:00<00:00, 212369.82it/s]\n",
      "Processing tokens: 100%|██████████| 33/33 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab7295a7ad2428aad3340d448cb1a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f245b4f243d4cc7ac8d9729ac1da7eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  20%|██        | 16/80 [01:19<05:09,  4.84s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 289/289 [00:00<00:00, 288608.06it/s]\n",
      "Processing tokens: 100%|██████████| 34/34 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9b5e2c13ea4d3e9bb465eb4b4ad5fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c2d7a8c39b4403aa110ac9c5525a418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  21%|██▏       | 17/80 [01:24<05:19,  5.07s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 18/18 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 78/78 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d1431c7b5a54f018d51b1a0e35c2a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e716a6f81a423d80e3809558ef5d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  22%|██▎       | 18/80 [01:28<04:39,  4.50s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 174/174 [00:00<00:00, 173888.23it/s]\n",
      "Processing tokens: 100%|██████████| 38/38 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f7ff36b0f147e3b0c6ce174a36c1f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46ccdba53eec41fe9d4d42d9f981b6a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  24%|██▍       | 19/80 [01:32<04:40,  4.60s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 69/69 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 47/47 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8297357c0e6f4a8296eba26a3d2007a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "335511258675465aa54619ad313e1648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  25%|██▌       | 20/80 [01:36<04:17,  4.29s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 91/91 [00:00<00:00, 90833.33it/s]\n",
      "Processing tokens: 100%|██████████| 43/43 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f10b80848444a95aaf6b8620f263a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba16791aa684f9aa9f39ce7a19bf180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  26%|██▋       | 21/80 [01:44<05:11,  5.28s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 12/12 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 29/29 [00:00<00:00, 29002.10it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9214baf303104014ac90d60f693412fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d13277e9968a4c4f9202ab0121e68f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  28%|██▊       | 22/80 [01:47<04:29,  4.65s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 321/321 [00:00<00:00, 320488.36it/s]\n",
      "Processing tokens: 100%|██████████| 35/35 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "147ab817ed97420591c83d644f313760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd65b6bd86f44ed29bd86e6ce86dc62d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  29%|██▉       | 23/80 [01:56<05:43,  6.02s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 64/64 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 21/21 [00:00<00:00, 21031.61it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40feeb9960684bbd8bff4840bf7480cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05bd1ad2a7c34864912986fb98071815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  30%|███       | 24/80 [02:00<04:56,  5.29s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 46/46 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 48/48 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea0751460cd4f23be62f0be3ac74a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1f59bed66c4dcebdfaaef27e4faeaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  31%|███▏      | 25/80 [02:04<04:28,  4.89s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 359/359 [00:00<00:00, 178703.43it/s]\n",
      "Processing tokens: 100%|██████████| 34/34 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f974473e4184d5f9b40a81bad93d669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2acf9bc94b34490ad0b046a4eb28f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  32%|███▎      | 26/80 [02:10<04:42,  5.23s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 207/207 [00:00<00:00, 206916.33it/s]\n",
      "Processing tokens: 100%|██████████| 44/44 [00:00<00:00, 44034.69it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aad87974af648a8a40c0427e32bfa7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ee2416c4b5460ebd885971553c4356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  34%|███▍      | 27/80 [02:14<04:31,  5.11s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 807/807 [00:00<00:00, 404155.62it/s]\n",
      "Processing tokens: 100%|██████████| 28/28 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309de1fe018f4d92a81c74b97796b87b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6103642fe34a61b6f413ae7917e681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  35%|███▌      | 28/80 [02:24<05:43,  6.60s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 581/581 [00:00<00:00, 290521.06it/s]\n",
      "Processing tokens: 100%|██████████| 39/39 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "733c05b7fe574de9b060b59937088628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5450e21fc904b76882835ff50d6570f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  36%|███▋      | 29/80 [02:32<05:54,  6.96s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 9/9 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 43/43 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d8ae57a6d3418eb1c977584dc88d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1a10f7ba9745b59558348f254401d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  38%|███▊      | 30/80 [02:35<04:51,  5.82s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 100/100 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 41/41 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5700fa0a16aa4e05943300b5f7e3cc8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60d57f3c5364a248a68bdc627a3e216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  39%|███▉      | 31/80 [02:39<04:16,  5.24s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 371/371 [00:00<00:00, 185336.68it/s]\n",
      "Processing tokens: 100%|██████████| 40/40 [00:00<00:00, 40127.28it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd44bcc0442248f7ab20ad6954b85172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "270e682963e0412ba168ee3553ad92f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  40%|████      | 32/80 [02:46<04:31,  5.65s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 261/261 [00:00<00:00, 261081.17it/s]\n",
      "Processing tokens: 100%|██████████| 35/35 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b161157513b4282bf4e06d424deeb34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b1f95dc834d481b8c867e8d9aa6bae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  41%|████▏     | 33/80 [02:51<04:22,  5.58s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 17/17 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 37/37 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a72d26832fff4065bc2b8764f354ef1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee7718082e114205a4ecc7a813d4293b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  42%|████▎     | 34/80 [02:54<03:43,  4.85s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 13/13 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 86/86 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4490fd19178e4e0da9616ef0f7e78687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a4cb1d8aa84cacacd8229b95dbd921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  44%|████▍     | 35/80 [02:58<03:14,  4.33s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 34/34 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 39/39 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "593f406021e544e3ace8eca8a6cbf10e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81b29a5099be40f8842785c79c91c0d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  45%|████▌     | 36/80 [03:01<03:01,  4.13s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 22/22 [00:00<00:00, 22038.38it/s]\n",
      "Processing tokens: 100%|██████████| 227/227 [00:00<00:00, 226908.25it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff664b20bfa43eea48f4b9cb0e8274f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c9db8d7926d4f3ea8fd57336466af38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  46%|████▋     | 37/80 [03:05<02:49,  3.95s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 26/26 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 491/491 [00:00<00:00, 245722.86it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ff4be8337549f2a7d36ec931b38237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05303df80f5742c9b1dfcdf19646555f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  48%|████▊     | 38/80 [03:08<02:40,  3.82s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 26/26 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 133/133 [00:00<00:00, 133168.40it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2e33684cf284d818136ecbf1f208ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5d7a90f1d74ddfa269ec01714395a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  49%|████▉     | 39/80 [03:12<02:29,  3.65s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 17/17 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 74/74 [00:00<00:00, 73934.85it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d55655ec4d4623b40c6e64d25aa250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c2c6efd5f54c9d8edddf040264a9a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  50%|█████     | 40/80 [03:15<02:19,  3.49s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 57/57 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 45/45 [00:00<00:00, 86342.03it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d8dc6fd4913432fbff286c0a2f53c31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27469639bd5a49bbba7b012d338ce7c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  51%|█████▏    | 41/80 [03:18<02:16,  3.51s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 22/22 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 62/62 [00:00<00:00, 62019.28it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76951c2909684858b3c0424dfad31f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7b50c7b18a4384969f419f30f85a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  52%|█████▎    | 42/80 [03:21<02:09,  3.41s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 19/19 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 174/174 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b731e324ce4742a1d46edfdc1eb889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1550a73b8634f9cb6043f0c395354ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  54%|█████▍    | 43/80 [03:25<02:04,  3.36s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 18/18 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 99/99 [00:00<00:00, 98889.28it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc0c1fb54784cfc9bb3b03c29467947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9f4ac5624b44f69dec697e995c3c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  55%|█████▌    | 44/80 [03:28<01:57,  3.27s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 24/24 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 144/144 [00:00<00:00, 144010.44it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7fe49ffe16b46bb906b62a0f59669e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "024bbce093ce40afaaa9c89c0c80ba54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  56%|█████▋    | 45/80 [03:31<01:54,  3.27s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 56/56 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 246/246 [00:00<00:00, 245549.45it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a51b849893047a8978edf08ba39a928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d2be4b0799470ea26be2f7cca03c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  57%|█████▊    | 46/80 [03:34<01:52,  3.32s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 152/152 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 80/80 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec57294dddf41e3801ec31074afec5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a55e23bca7464bf985c24434aa74c734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  59%|█████▉    | 47/80 [03:39<02:01,  3.68s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 145/145 [00:00<00:00, 145079.69it/s]\n",
      "Processing tokens: 100%|██████████| 54/54 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c8a496b55644e4a67d1e04b46d480a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc3b72549a94207937123460b86e32a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  60%|██████    | 48/80 [03:43<02:05,  3.94s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 306/306 [00:00<00:00, 198893.08it/s]\n",
      "Processing tokens: 100%|██████████| 40/40 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c48a614d7a4fd4afce3fa5515af0e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09dece45bac43e2963324657a2e2519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  61%|██████▏   | 49/80 [03:49<02:18,  4.47s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 25/25 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 37/37 [00:00<00:00, 37046.85it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a27dca4c44a5427daeb43a0683e11bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c8f707f3f4d48d388aa8e10d471d283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  62%|██████▎   | 50/80 [03:52<02:01,  4.06s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 16/16 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 8/8 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baebde4d404e46aa9b92ccf29288ff47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aeca88a82f94d56b818ddc1942e7073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  64%|██████▍   | 51/80 [03:55<01:49,  3.79s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 17/17 [00:00<00:00, 16993.13it/s]\n",
      "Processing tokens: 100%|██████████| 8/8 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f0c8134cf84b549634debbde3a548a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "974f3d217f484cffb7da6ba250441ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  65%|██████▌   | 52/80 [03:59<01:41,  3.61s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 44/44 [00:00<00:00, 44045.20it/s]\n",
      "Processing tokens: 100%|██████████| 8/8 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a2edf6ba05a4f6bb9ddd76af5e94a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca5f1dc76984423992d505df7dba343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  66%|██████▋   | 53/80 [04:02<01:35,  3.52s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 17/17 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 8/8 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b0d45fc6334edcbf254aa3f554fed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf0b6329b2e4f8a9c560e8886b13d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  68%|██████▊   | 54/80 [04:05<01:28,  3.42s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 28/28 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 8/8 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664e62c84a914bf091f087cd44edfdf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8536c76c7f4949db99f2f0d33a1dd670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  69%|██████▉   | 55/80 [04:08<01:24,  3.38s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 19/19 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 8/8 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61fbee24056143a594605ee3c44265fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd07603336d4fdb958e3b257ae4a1ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  70%|███████   | 56/80 [04:12<01:19,  3.32s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 30/30 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 8/8 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c2d84b2c654b328cce7ba9f3f473a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "201a0aa282c44789b16e11a68f053084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  71%|███████▏  | 57/80 [04:15<01:15,  3.29s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 38/38 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 8/8 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a010749d6c4e60a27665fb1d24bd17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b10315872d43dc81e326aafa519cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  72%|███████▎  | 58/80 [04:18<01:12,  3.29s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 27/27 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 8/8 [00:00<00:00, 7994.86it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5353f9b2b6644fe846d09c0b70386ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e03bd023ae4d6ba1e880729687cbe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  74%|███████▍  | 59/80 [04:21<01:08,  3.26s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 19/19 [00:00<00:00, 19042.24it/s]\n",
      "Processing tokens: 100%|██████████| 8/8 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85de28baba6a4784b73f7eb18b0d0ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6778efe09e443a399563ead150b160e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  75%|███████▌  | 60/80 [04:28<01:23,  4.20s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 12/12 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 8/8 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57de933536604901b4e064e32b8564ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e81f9ba13f486088caf044d9d5672e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  76%|███████▋  | 61/80 [04:31<01:15,  3.96s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 19/19 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 8/8 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "469e269c49704ac79024321c13732e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b320164ca1204dfd9d7ec561552b0e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  78%|███████▊  | 62/80 [04:38<01:25,  4.74s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 12/12 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 8/8 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace9ca376b2042a8932c494d16baa973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a78d02256a486eb9ff8e5f75a50407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  79%|███████▉  | 63/80 [04:41<01:11,  4.22s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 21/21 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 8/8 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6167e24ff96c40f8b7303a82173d4a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8737b945c444f7ba5e3bc46db47c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  80%|████████  | 64/80 [04:44<01:02,  3.90s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 28/28 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 8/8 [00:00<00:00, 8008.22it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "104af47dcfee470f99df68e1af325629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f2667a9b97414f9037ca2d865c1d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  81%|████████▏ | 65/80 [04:47<00:55,  3.69s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 33/33 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 8/8 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6559d276c593456083631f961ac99881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47cff88eef264ab69c8c652d2c300ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  82%|████████▎ | 66/80 [04:50<00:49,  3.57s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 37/37 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 8/8 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0fcc639de514432b3a46687ea6769ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcc82a86d6ae4ec286eb6f6fce25f004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  84%|████████▍ | 67/80 [04:54<00:45,  3.49s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 56/56 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 35/35 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099a39a073ac46efab5d4d1ecf9dff40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03024fe58ea744508f3f69681f62e3a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  85%|████████▌ | 68/80 [04:58<00:43,  3.64s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 104/104 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 47/47 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8471545b1c134fc8a191dc81d4a8afab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b3c91baccd441bc9646d35f309791e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  86%|████████▋ | 69/80 [05:02<00:41,  3.81s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 25/25 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 8/8 [00:00<00:00, 7992.96it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22246cd6531b487a971b1ce2bd87e37a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b771fe9d72ad404f9e2944111c6efb38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  88%|████████▊ | 70/80 [05:08<00:46,  4.60s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 29/29 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 45/45 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcda95cf184c438aa091071edc4ac589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3784f029de014035824b6fa680d01cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  89%|████████▉ | 71/80 [05:12<00:38,  4.24s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 19/19 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 39/39 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f82a149a1145518c185ef51e978ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1564661b60ed4a799708281513ea5261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  90%|█████████ | 72/80 [05:15<00:31,  3.90s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 31/31 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 8/8 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821c27ee39724a65a031a8b80d882651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac0ebb14a2b489d821c51a0dc256f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  91%|█████████▏| 73/80 [05:18<00:26,  3.77s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 31/31 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 8/8 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ad53311d7b462c8c7f7f2a8f5fd0ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe78fde74d8648058900b49b18cde393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  92%|█████████▎| 74/80 [05:21<00:21,  3.59s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 21/21 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 8/8 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d449ddd4ea8d4336940d7a69bfecbc2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f30690fd7f4ecdb72f8a498de5452f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  94%|█████████▍| 75/80 [05:28<00:22,  4.48s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 131/131 [00:00<00:00, 131322.62it/s]\n",
      "Processing tokens: 100%|██████████| 260/260 [00:00<00:00, 260080.86it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a956b4ffce854123b15adff32e0cb7bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82aa1ff885d94724a8592f7b22e1ce5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  95%|█████████▌| 76/80 [05:32<00:17,  4.40s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 42/42 [00:00<00:00, 42053.18it/s]\n",
      "Processing tokens: 100%|██████████| 33/33 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800938e4ada840da9f659eb0b6fcb03d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3666b2acaa184a69b650225edc173444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  96%|█████████▋| 77/80 [05:36<00:12,  4.09s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 41/41 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 32/32 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360e935915284a6eb13ebaca71c1d944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8debdb19eeb4737aa1383bf9599c099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  98%|█████████▊| 78/80 [05:39<00:07,  3.92s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 19/19 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 8/8 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0224c5f9ea014d5cbca4736589c41ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb9cdee914df4f98baceef14ff546bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  99%|█████████▉| 79/80 [05:42<00:03,  3.69s/it]INFO:absl:Using default tokenizer.\n",
      "Processing tokens: 100%|██████████| 27/27 [00:00<?, ?it/s]\n",
      "Processing tokens: 100%|██████████| 36/36 [00:00<?, ?it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4273b9fa8b4b47f0becc5ef3475828e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f7452170264d4ab6c56b47ad429ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    }
   ],
   "source": [
    "# First pass to collect metric values and determine min and max for normalization\n",
    "for index, row in tqdm(data.iterrows(), total=len(data), desc=\"Processing questions\", leave=False):\n",
    "    question = row[\"Question\"]\n",
    "    reference_answer = row[\"Answer\"]\n",
    "    predicted_answer = answer_query(question)\n",
    "    predicted_answers.append(predicted_answer)\n",
    "\n",
    "    # Evaluate the metrics\n",
    "    rouge = evaluate_rouge(predicted_answer, reference_answer)\n",
    "    bleu = evaluate_bleu(predicted_answer, reference_answer)\n",
    "    f1 = evaluate_f1(predicted_answer, reference_answer)\n",
    "    bert_score = evaluate_bert_score(predicted_answer, reference_answer)\n",
    "    embedding_similarity = evaluate_embedding_similarity(predicted_answer, reference_answer)\n",
    "\n",
    "    # Update min and max values for each metric\n",
    "    rouge_scores.append(rouge[\"rougeL\"].fmeasure)\n",
    "    bleu_scores.append(bleu)\n",
    "    f1_scores.append(f1)\n",
    "    bert_scores.append(bert_score)\n",
    "    embedding_similarities.append(embedding_similarity)\n",
    "\n",
    "    rouge_min, rouge_max = min(rouge_min, rouge[\"rougeL\"].fmeasure), max(rouge_max, rouge[\"rougeL\"].fmeasure)\n",
    "    bleu_min, bleu_max = min(bleu_min, bleu), max(bleu_max, bleu)\n",
    "    f1_min, f1_max = min(f1_min, f1), max(f1_max, f1)\n",
    "    bert_min, bert_max = min(bert_min, bert_score), max(bert_max, bert_score)\n",
    "    embedding_min, embedding_max = min(embedding_min, embedding_similarity), max(embedding_max, embedding_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating hybrid scores: 100%|██████████| 80/80 [00:00<00:00, 20012.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Processing question 1]:\n",
      "\n",
      "ROUGE-L: 0.14457831325301204\n",
      "BLEU: 2.1562467214096475\n",
      "F1: 0.19607843137254904\n",
      "BERT Score: 0.8371809124946594\n",
      "Embedding Similarity: 0.7557029128074646\n",
      "Hybrid Score: 0.40106731357230574\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 2]:\n",
      "\n",
      "ROUGE-L: 0.23148148148148145\n",
      "BLEU: 2.99386624213049\n",
      "F1: 0.2201834862385321\n",
      "BERT Score: 0.8727231621742249\n",
      "Embedding Similarity: 0.770326554775238\n",
      "Hybrid Score: 0.5546326071605248\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 3]:\n",
      "\n",
      "ROUGE-L: 0.1323529411764706\n",
      "BLEU: 1.17170371353362\n",
      "F1: 0.16279069767441862\n",
      "BERT Score: 0.8237793445587158\n",
      "Embedding Similarity: 0.5661188364028931\n",
      "Hybrid Score: 0.29283189566480866\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 4]:\n",
      "\n",
      "ROUGE-L: 0.19718309859154928\n",
      "BLEU: 0.8617830041262791\n",
      "F1: 0.2\n",
      "BERT Score: 0.8569048643112183\n",
      "Embedding Similarity: 0.5302807092666626\n",
      "Hybrid Score: 0.40505421813310977\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 5]:\n",
      "\n",
      "ROUGE-L: 0.2110091743119266\n",
      "BLEU: 1.4521673571898857\n",
      "F1: 0.2113821138211382\n",
      "BERT Score: 0.8437254428863525\n",
      "Embedding Similarity: 0.6106436252593994\n",
      "Hybrid Score: 0.4187299732372002\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 6]:\n",
      "\n",
      "ROUGE-L: 0.09090909090909091\n",
      "BLEU: 0.9167838327954699\n",
      "F1: 0.0979020979020979\n",
      "BERT Score: 0.835375964641571\n",
      "Embedding Similarity: 0.6834882497787476\n",
      "Hybrid Score: 0.2889248933488796\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 7]:\n",
      "\n",
      "ROUGE-L: 0.23140495867768593\n",
      "BLEU: 3.2580119882224188\n",
      "F1: 0.3518518518518519\n",
      "BERT Score: 0.8367453217506409\n",
      "Embedding Similarity: 0.6459866762161255\n",
      "Hybrid Score: 0.5072365096000044\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 8]:\n",
      "\n",
      "ROUGE-L: 0.21548821548821548\n",
      "BLEU: 5.520348459169549\n",
      "F1: 0.38461538461538464\n",
      "BERT Score: 0.8549061417579651\n",
      "Embedding Similarity: 0.8497692346572876\n",
      "Hybrid Score: 0.6275364675506607\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 9]:\n",
      "\n",
      "ROUGE-L: 0.11799410029498525\n",
      "BLEU: 4.188525828486855\n",
      "F1: 0.11475409836065574\n",
      "BERT Score: 0.8442542552947998\n",
      "Embedding Similarity: 0.5841661691665649\n",
      "Hybrid Score: 0.3509293961214721\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 10]:\n",
      "\n",
      "ROUGE-L: 0.09345794392523364\n",
      "BLEU: 0.4310764544910433\n",
      "F1: 0.12698412698412698\n",
      "BERT Score: 0.8370015621185303\n",
      "Embedding Similarity: 0.3528570532798767\n",
      "Hybrid Score: 0.21894937576124746\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 11]:\n",
      "\n",
      "ROUGE-L: 0.2537562604340568\n",
      "BLEU: 15.008487856667655\n",
      "F1: 0.49113924050632907\n",
      "BERT Score: 0.8708090782165527\n",
      "Embedding Similarity: 0.9182280898094177\n",
      "Hybrid Score: 0.8713257305802384\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 12]:\n",
      "\n",
      "ROUGE-L: 0.06462035541195477\n",
      "BLEU: 0.40609034319956117\n",
      "F1: 0.05620608899297423\n",
      "BERT Score: 0.8063996434211731\n",
      "Embedding Similarity: 0.4870741367340088\n",
      "Hybrid Score: 0.1407601830546434\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 13]:\n",
      "\n",
      "ROUGE-L: 0.21138211382113822\n",
      "BLEU: 9.078128831210421\n",
      "F1: 0.4444444444444445\n",
      "BERT Score: 0.8607703447341919\n",
      "Embedding Similarity: 0.8658548593521118\n",
      "Hybrid Score: 0.7126947511536212\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 14]:\n",
      "\n",
      "ROUGE-L: 0.18461538461538463\n",
      "BLEU: 1.3799403367404997\n",
      "F1: 0.21621621621621623\n",
      "BERT Score: 0.8534121513366699\n",
      "Embedding Similarity: 0.46810466051101685\n",
      "Hybrid Score: 0.3881956875969864\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 15]:\n",
      "\n",
      "ROUGE-L: 0.10810810810810811\n",
      "BLEU: 1.8777510442160874\n",
      "F1: 0.18181818181818185\n",
      "BERT Score: 0.8311554789543152\n",
      "Embedding Similarity: 0.3655642867088318\n",
      "Hybrid Score: 0.2609587008239009\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 16]:\n",
      "\n",
      "ROUGE-L: 0.10196078431372549\n",
      "BLEU: 3.442512241341717\n",
      "F1: 0.13924050632911394\n",
      "BERT Score: 0.8337273001670837\n",
      "Embedding Similarity: 0.6066457033157349\n",
      "Hybrid Score: 0.3244202011694088\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 17]:\n",
      "\n",
      "ROUGE-L: 0.09876543209876543\n",
      "BLEU: 1.895596612335734\n",
      "F1: 0.06511627906976744\n",
      "BERT Score: 0.822767972946167\n",
      "Embedding Similarity: 0.6288315057754517\n",
      "Hybrid Score: 0.2543409957616063\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 18]:\n",
      "\n",
      "ROUGE-L: 0.08080808080808081\n",
      "BLEU: 0.1571857560241017\n",
      "F1: 0.07017543859649122\n",
      "BERT Score: 0.8399695158004761\n",
      "Embedding Similarity: 0.672404408454895\n",
      "Hybrid Score: 0.2678323349917684\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 19]:\n",
      "\n",
      "ROUGE-L: 0.15023474178403756\n",
      "BLEU: 1.072398978139567\n",
      "F1: 0.15151515151515152\n",
      "BERT Score: 0.8498275876045227\n",
      "Embedding Similarity: 0.7620948553085327\n",
      "Hybrid Score: 0.3997612722131505\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 20]:\n",
      "\n",
      "ROUGE-L: 0.2542372881355932\n",
      "BLEU: 3.9186962126746825\n",
      "F1: 0.29729729729729726\n",
      "BERT Score: 0.8570894002914429\n",
      "Embedding Similarity: 0.7036118507385254\n",
      "Hybrid Score: 0.5644489346764763\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 21]:\n",
      "\n",
      "ROUGE-L: 0.18840579710144925\n",
      "BLEU: 1.9585353625856992\n",
      "F1: 0.16091954022988506\n",
      "BERT Score: 0.8402339220046997\n",
      "Embedding Similarity: 0.6497942805290222\n",
      "Hybrid Score: 0.39275599629343516\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 22]:\n",
      "\n",
      "ROUGE-L: 0.08888888888888889\n",
      "BLEU: 1.8706790066692034\n",
      "F1: 0.07692307692307693\n",
      "BERT Score: 0.8232764601707458\n",
      "Embedding Similarity: 0.5734400749206543\n",
      "Hybrid Score: 0.24008226826586843\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 23]:\n",
      "\n",
      "ROUGE-L: 0.09239130434782608\n",
      "BLEU: 0.9291433333903963\n",
      "F1: 0.06779661016949153\n",
      "BERT Score: 0.8078169226646423\n",
      "Embedding Similarity: 0.5844696760177612\n",
      "Hybrid Score: 0.19682722688468118\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 24]:\n",
      "\n",
      "ROUGE-L: 0.27906976744186046\n",
      "BLEU: 3.666600822373393\n",
      "F1: 0.34782608695652173\n",
      "BERT Score: 0.89006108045578\n",
      "Embedding Similarity: 0.8403691053390503\n",
      "Hybrid Score: 0.6988661753230903\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 25]:\n",
      "\n",
      "ROUGE-L: 0.2127659574468085\n",
      "BLEU: 2.9707159749433516\n",
      "F1: 0.19607843137254902\n",
      "BERT Score: 0.865287721157074\n",
      "Embedding Similarity: 0.7525055408477783\n",
      "Hybrid Score: 0.5128412629872985\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 26]:\n",
      "\n",
      "ROUGE-L: 0.07575757575757575\n",
      "BLEU: 0.8062760525022523\n",
      "F1: 0.08\n",
      "BERT Score: 0.8267209529876709\n",
      "Embedding Similarity: 0.5285779237747192\n",
      "Hybrid Score: 0.21495146819599953\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 27]:\n",
      "\n",
      "ROUGE-L: 0.12307692307692306\n",
      "BLEU: 1.5974567194177718\n",
      "F1: 0.20606060606060606\n",
      "BERT Score: 0.8577601313591003\n",
      "Embedding Similarity: 0.8029632568359375\n",
      "Hybrid Score: 0.43788176051994987\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 28]:\n",
      "\n",
      "ROUGE-L: 0.04678362573099415\n",
      "BLEU: 0.21396266661942936\n",
      "F1: 0.05474452554744526\n",
      "BERT Score: 0.8291760087013245\n",
      "Embedding Similarity: 0.6215180158615112\n",
      "Hybrid Score: 0.20586677747294593\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 29]:\n",
      "\n",
      "ROUGE-L: 0.12025316455696203\n",
      "BLEU: 3.0134770751726903\n",
      "F1: 0.10741687979539642\n",
      "BERT Score: 0.8464378714561462\n",
      "Embedding Similarity: 0.7593129873275757\n",
      "Hybrid Score: 0.3807937104582436\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 30]:\n",
      "\n",
      "ROUGE-L: 0.03773584905660377\n",
      "BLEU: 0.12970747326593035\n",
      "F1: 0.0\n",
      "BERT Score: 0.8148314356803894\n",
      "Embedding Similarity: 0.1077980250120163\n",
      "Hybrid Score: 0.022223262885678177\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 31]:\n",
      "\n",
      "ROUGE-L: 0.12162162162162161\n",
      "BLEU: 1.2990384578380083\n",
      "F1: 0.0963855421686747\n",
      "BERT Score: 0.8202390074729919\n",
      "Embedding Similarity: 0.7111027240753174\n",
      "Hybrid Score: 0.28853252462506795\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 32]:\n",
      "\n",
      "ROUGE-L: 0.10747663551401869\n",
      "BLEU: 0.7242834151475019\n",
      "F1: 0.08480565371024736\n",
      "BERT Score: 0.8148952722549438\n",
      "Embedding Similarity: 0.6444076895713806\n",
      "Hybrid Score: 0.23986154723753467\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 33]:\n",
      "\n",
      "ROUGE-L: 0.0792079207920792\n",
      "BLEU: 0.37598880280347774\n",
      "F1: 0.06896551724137931\n",
      "BERT Score: 0.8079591393470764\n",
      "Embedding Similarity: 0.7920136451721191\n",
      "Hybrid Score: 0.2322092725042935\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 34]:\n",
      "\n",
      "ROUGE-L: 0.10344827586206896\n",
      "BLEU: 1.8128215103086287\n",
      "F1: 0.06250000000000001\n",
      "BERT Score: 0.8402506709098816\n",
      "Embedding Similarity: 0.5877825021743774\n",
      "Hybrid Score: 0.28128974332054274\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 35]:\n",
      "\n",
      "ROUGE-L: 0.05999999999999999\n",
      "BLEU: 0.025687795588920447\n",
      "F1: 0.04081632653061224\n",
      "BERT Score: 0.8326779007911682\n",
      "Embedding Similarity: 0.4508715271949768\n",
      "Hybrid Score: 0.17191797402184603\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 36]:\n",
      "\n",
      "ROUGE-L: 0.19178082191780824\n",
      "BLEU: 4.364246200927671\n",
      "F1: 0.12121212121212122\n",
      "BERT Score: 0.870262086391449\n",
      "Embedding Similarity: 0.41230282187461853\n",
      "Hybrid Score: 0.4150931088526814\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 37]:\n",
      "\n",
      "ROUGE-L: 0.078125\n",
      "BLEU: 0.0017998862324773955\n",
      "F1: 0.10071942446043165\n",
      "BERT Score: 0.8078888058662415\n",
      "Embedding Similarity: 0.6584353446960449\n",
      "Hybrid Score: 0.20685901864753414\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 38]:\n",
      "\n",
      "ROUGE-L: 0.0599250936329588\n",
      "BLEU: 9.827492019647251e-08\n",
      "F1: 0.07662835249042145\n",
      "BERT Score: 0.8120280504226685\n",
      "Embedding Similarity: 0.5894907712936401\n",
      "Hybrid Score: 0.1771615513502065\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 39]:\n",
      "\n",
      "ROUGE-L: 0.15\n",
      "BLEU: 0.15073793574659267\n",
      "F1: 0.15217391304347827\n",
      "BERT Score: 0.8483090996742249\n",
      "Embedding Similarity: 0.5792456269264221\n",
      "Hybrid Score: 0.3400352429951737\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 40]:\n",
      "\n",
      "ROUGE-L: 0.08602150537634408\n",
      "BLEU: 0.11454233587734834\n",
      "F1: 0.04651162790697675\n",
      "BERT Score: 0.8180989623069763\n",
      "Embedding Similarity: 0.21712926030158997\n",
      "Hybrid Score: 0.10519764094431633\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 41]:\n",
      "\n",
      "ROUGE-L: 0.19230769230769232\n",
      "BLEU: 2.355420091257551\n",
      "F1: 0.3076923076923077\n",
      "BERT Score: 0.861883282661438\n",
      "Embedding Similarity: 0.40912190079689026\n",
      "Hybrid Score: 0.4465289064105669\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 42]:\n",
      "\n",
      "ROUGE-L: 0.09195402298850576\n",
      "BLEU: 0.4591485857887379\n",
      "F1: 0.12\n",
      "BERT Score: 0.844323992729187\n",
      "Embedding Similarity: 0.4540991187095642\n",
      "Hybrid Score: 0.255231760858535\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 43]:\n",
      "\n",
      "ROUGE-L: 0.05025125628140703\n",
      "BLEU: 0.002252798558703892\n",
      "F1: 0.07407407407407408\n",
      "BERT Score: 0.8230432868003845\n",
      "Embedding Similarity: 0.44598162174224854\n",
      "Hybrid Score: 0.15782685987570338\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 44]:\n",
      "\n",
      "ROUGE-L: 0.08403361344537816\n",
      "BLEU: 0.051965027692995304\n",
      "F1: 0.08450704225352113\n",
      "BERT Score: 0.8402783274650574\n",
      "Embedding Similarity: 0.4321538507938385\n",
      "Hybrid Score: 0.21659545420417411\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 45]:\n",
      "\n",
      "ROUGE-L: 0.15204678362573099\n",
      "BLEU: 0.1536518537137157\n",
      "F1: 0.16326530612244897\n",
      "BERT Score: 0.8536517024040222\n",
      "Embedding Similarity: 0.6642191410064697\n",
      "Hybrid Score: 0.37757719092037206\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 46]:\n",
      "\n",
      "ROUGE-L: 0.16233766233766234\n",
      "BLEU: 0.9679978790270559\n",
      "F1: 0.13422818791946312\n",
      "BERT Score: 0.838639497756958\n",
      "Embedding Similarity: 0.6108138561248779\n",
      "Hybrid Score: 0.3392282142935996\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 47]:\n",
      "\n",
      "ROUGE-L: 0.19658119658119658\n",
      "BLEU: 3.2851631455095895\n",
      "F1: 0.2222222222222222\n",
      "BERT Score: 0.837953507900238\n",
      "Embedding Similarity: 0.7879160642623901\n",
      "Hybrid Score: 0.4694860489606666\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 48]:\n",
      "\n",
      "ROUGE-L: 0.20792079207920794\n",
      "BLEU: 1.8253518006191725\n",
      "F1: 0.23703703703703705\n",
      "BERT Score: 0.8299038410186768\n",
      "Embedding Similarity: 0.8789339661598206\n",
      "Hybrid Score: 0.4688254815235353\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 49]:\n",
      "\n",
      "ROUGE-L: 0.07282913165266106\n",
      "BLEU: 0.319552352017376\n",
      "F1: 0.05405405405405405\n",
      "BERT Score: 0.823250412940979\n",
      "Embedding Similarity: 0.66676926612854\n",
      "Hybrid Score: 0.22243624857381797\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 50]:\n",
      "\n",
      "ROUGE-L: 0.09374999999999999\n",
      "BLEU: 1.1685792820286083\n",
      "F1: 0.052631578947368425\n",
      "BERT Score: 0.8438579440116882\n",
      "Embedding Similarity: 0.7012748122215271\n",
      "Hybrid Score: 0.2974969508828424\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 51]:\n",
      "\n",
      "ROUGE-L: 0.16666666666666669\n",
      "BLEU: 3.211547431691929\n",
      "F1: 0.4\n",
      "BERT Score: 0.901445746421814\n",
      "Embedding Similarity: 0.28511956334114075\n",
      "Hybrid Score: 0.5306485481855783\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 52]:\n",
      "\n",
      "ROUGE-L: 0.24000000000000005\n",
      "BLEU: 3.0272532566104675\n",
      "F1: 0.36363636363636365\n",
      "BERT Score: 0.9032790660858154\n",
      "Embedding Similarity: 0.2613670229911804\n",
      "Hybrid Score: 0.558367128521843\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 53]:\n",
      "\n",
      "ROUGE-L: 0.15384615384615385\n",
      "BLEU: 1.2165013609501771\n",
      "F1: 0.16\n",
      "BERT Score: 0.8674108386039734\n",
      "Embedding Similarity: 0.10710060596466064\n",
      "Hybrid Score: 0.2846036297593939\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 54]:\n",
      "\n",
      "ROUGE-L: 0.14814814814814814\n",
      "BLEU: 2.664321121388895\n",
      "F1: 0.14285714285714288\n",
      "BERT Score: 0.8711292743682861\n",
      "Embedding Similarity: 0.18084830045700073\n",
      "Hybrid Score: 0.31886170221017984\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 55]:\n",
      "\n",
      "ROUGE-L: 0.16216216216216214\n",
      "BLEU: 1.6961343903963388\n",
      "F1: 0.21052631578947367\n",
      "BERT Score: 0.8737186789512634\n",
      "Embedding Similarity: 0.14346426725387573\n",
      "Hybrid Score: 0.33875393635804424\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 56]:\n",
      "\n",
      "ROUGE-L: 0.21428571428571427\n",
      "BLEU: 4.343737891358309\n",
      "F1: 0.3333333333333333\n",
      "BERT Score: 0.8979343771934509\n",
      "Embedding Similarity: 0.09771109372377396\n",
      "Hybrid Score: 0.49630333350243006\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 57]:\n",
      "\n",
      "ROUGE-L: 0.10256410256410256\n",
      "BLEU: 1.2557690800697192\n",
      "F1: 0.08695652173913045\n",
      "BERT Score: 0.8340345621109009\n",
      "Embedding Similarity: 0.14301913976669312\n",
      "Hybrid Score: 0.16236158162673633\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 58]:\n",
      "\n",
      "ROUGE-L: 0.13043478260869568\n",
      "BLEU: 1.3389812472744242\n",
      "F1: 0.16666666666666666\n",
      "BERT Score: 0.8706835508346558\n",
      "Embedding Similarity: 0.12069476395845413\n",
      "Hybrid Score: 0.28401521466145935\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 59]:\n",
      "\n",
      "ROUGE-L: 0.17142857142857146\n",
      "BLEU: 1.8196871111910016\n",
      "F1: 0.1111111111111111\n",
      "BERT Score: 0.8926060199737549\n",
      "Embedding Similarity: 0.09508861601352692\n",
      "Hybrid Score: 0.3330893467939285\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 60]:\n",
      "\n",
      "ROUGE-L: 0.2222222222222222\n",
      "BLEU: 2.5683319547529764\n",
      "F1: 0.30769230769230765\n",
      "BERT Score: 0.8904070854187012\n",
      "Embedding Similarity: 0.23515500128269196\n",
      "Hybrid Score: 0.48514193404713546\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 61]:\n",
      "\n",
      "ROUGE-L: 0.09523809523809525\n",
      "BLEU: 3.673526562988939\n",
      "F1: 0.2\n",
      "BERT Score: 0.8865282535552979\n",
      "Embedding Similarity: 0.13908405601978302\n",
      "Hybrid Score: 0.3433341095889342\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 62]:\n",
      "\n",
      "ROUGE-L: 0.14814814814814814\n",
      "BLEU: 2.352622489487909\n",
      "F1: 0.36363636363636365\n",
      "BERT Score: 0.8968977332115173\n",
      "Embedding Similarity: 0.1955832540988922\n",
      "Hybrid Score: 0.4613901657716917\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 63]:\n",
      "\n",
      "ROUGE-L: 0.09523809523809525\n",
      "BLEU: 4.065425428798724\n",
      "F1: 0.2222222222222222\n",
      "BERT Score: 0.8870591521263123\n",
      "Embedding Similarity: 0.15752984583377838\n",
      "Hybrid Score: 0.3631835401019866\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 64]:\n",
      "\n",
      "ROUGE-L: 0.06666666666666667\n",
      "BLEU: 1.8709718017288024\n",
      "F1: 0.11764705882352941\n",
      "BERT Score: 0.8585828542709351\n",
      "Embedding Similarity: 0.195379376411438\n",
      "Hybrid Score: 0.22346573911573508\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 65]:\n",
      "\n",
      "ROUGE-L: 0.10810810810810811\n",
      "BLEU: 1.6961343903963388\n",
      "F1: 0.09523809523809525\n",
      "BERT Score: 0.8593633770942688\n",
      "Embedding Similarity: 0.12404292821884155\n",
      "Hybrid Score: 0.2228317032412978\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 66]:\n",
      "\n",
      "ROUGE-L: 0.0975609756097561\n",
      "BLEU: 1.3494116947566301\n",
      "F1: 0.09523809523809525\n",
      "BERT Score: 0.8712306618690491\n",
      "Embedding Similarity: 0.135515958070755\n",
      "Hybrid Score: 0.23874279492659867\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 67]:\n",
      "\n",
      "ROUGE-L: 0.13333333333333333\n",
      "BLEU: 1.3389812472744242\n",
      "F1: 0.18181818181818182\n",
      "BERT Score: 0.863311767578125\n",
      "Embedding Similarity: 0.16180282831192017\n",
      "Hybrid Score: 0.28681128057014516\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 68]:\n",
      "\n",
      "ROUGE-L: 0.2978723404255319\n",
      "BLEU: 5.097805140619235\n",
      "F1: 0.30769230769230765\n",
      "BERT Score: 0.8615278601646423\n",
      "Embedding Similarity: 0.8259215354919434\n",
      "Hybrid Score: 0.651222746089089\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 69]:\n",
      "\n",
      "ROUGE-L: 0.18987341772151897\n",
      "BLEU: 2.0163865463453767\n",
      "F1: 0.17204301075268816\n",
      "BERT Score: 0.8516159653663635\n",
      "Embedding Similarity: 0.773298442363739\n",
      "Hybrid Score: 0.4525019734778424\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 70]:\n",
      "\n",
      "ROUGE-L: 0.1764705882352941\n",
      "BLEU: 2.075273865705644\n",
      "F1: 0.23529411764705882\n",
      "BERT Score: 0.8881016969680786\n",
      "Embedding Similarity: 0.16684956848621368\n",
      "Hybrid Score: 0.39843103237973115\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 71]:\n",
      "\n",
      "ROUGE-L: 0.10256410256410255\n",
      "BLEU: 1.9501082243126682\n",
      "F1: 0.0851063829787234\n",
      "BERT Score: 0.8336938619613647\n",
      "Embedding Similarity: 0.4353347420692444\n",
      "Hybrid Score: 0.2411820100722673\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 72]:\n",
      "\n",
      "ROUGE-L: 0.13559322033898302\n",
      "BLEU: 2.6955184898964144\n",
      "F1: 0.1111111111111111\n",
      "BERT Score: 0.8745329976081848\n",
      "Embedding Similarity: 0.5693799257278442\n",
      "Hybrid Score: 0.39973780099671874\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 73]:\n",
      "\n",
      "ROUGE-L: 0.09999999999999999\n",
      "BLEU: 1.2557690800697192\n",
      "F1: 0.0\n",
      "BERT Score: 0.8531844019889832\n",
      "Embedding Similarity: 0.10351748764514923\n",
      "Hybrid Score: 0.15524473211495757\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 74]:\n",
      "\n",
      "ROUGE-L: 0.15000000000000002\n",
      "BLEU: 1.6794256847485505\n",
      "F1: 0.18181818181818182\n",
      "BERT Score: 0.8743623495101929\n",
      "Embedding Similarity: 0.20043806731700897\n",
      "Hybrid Score: 0.3342230463477148\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 75]:\n",
      "\n",
      "ROUGE-L: 0.2\n",
      "BLEU: 2.273154356702287\n",
      "F1: 0.11764705882352941\n",
      "BERT Score: 0.8657786846160889\n",
      "Embedding Similarity: 0.19626817107200623\n",
      "Hybrid Score: 0.32929410826057787\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 76]:\n",
      "\n",
      "ROUGE-L: 0.1795511221945137\n",
      "BLEU: 0.6510977100951277\n",
      "F1: 0.17213114754098363\n",
      "BERT Score: 0.8310340046882629\n",
      "Embedding Similarity: 0.7472428679466248\n",
      "Hybrid Score: 0.3789124229500662\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 77]:\n",
      "\n",
      "ROUGE-L: 0.35000000000000003\n",
      "BLEU: 12.268454823008192\n",
      "F1: 0.39999999999999997\n",
      "BERT Score: 0.8761551380157471\n",
      "Embedding Similarity: 0.45497894287109375\n",
      "Hybrid Score: 0.7578215943304031\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 78]:\n",
      "\n",
      "ROUGE-L: 0.18181818181818182\n",
      "BLEU: 1.133052865607491\n",
      "F1: 0.186046511627907\n",
      "BERT Score: 0.8572712540626526\n",
      "Embedding Similarity: 0.5384351015090942\n",
      "Hybrid Score: 0.39588370487760316\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 79]:\n",
      "\n",
      "ROUGE-L: 0.14814814814814814\n",
      "BLEU: 2.707657626755448\n",
      "F1: 0.36363636363636365\n",
      "BERT Score: 0.8976147770881653\n",
      "Embedding Similarity: 0.16082815825939178\n",
      "Hybrid Score: 0.4591570492399603\n",
      "--------------------------------------------------------------------------------\n",
      "[Processing question 80]:\n",
      "\n",
      "ROUGE-L: 0.208955223880597\n",
      "BLEU: 7.018651055093765\n",
      "F1: 0.23255813953488372\n",
      "BERT Score: 0.865936815738678\n",
      "Embedding Similarity: 0.7639992237091064\n",
      "Hybrid Score: 0.5833302845631598\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Second pass to normalize the metrics and calculate the hybrid score\n",
    "for index in tqdm(range(len(data)), desc=\"Calculating hybrid scores\"):\n",
    "    rouge = rouge_scores[index]\n",
    "    bleu = bleu_scores[index]\n",
    "    f1 = f1_scores[index]\n",
    "    bert_score = bert_scores[index]\n",
    "    embedding_similarity = embedding_similarities[index]\n",
    "\n",
    "    # Normalize each metric\n",
    "    rouge_normalized = (rouge - rouge_min) / (rouge_max - rouge_min) if rouge_max != rouge_min else 0.5\n",
    "    bleu_normalized = (bleu - bleu_min) / (bleu_max - bleu_min) if bleu_max != bleu_min else 0.5\n",
    "    f1_normalized = (f1 - f1_min) / (f1_max - f1_min) if f1_max != f1_min else 0.5\n",
    "    bert_normalized = (bert_score - bert_min) / (bert_max - bert_min) if bert_max != bert_min else 0.5\n",
    "    embedding_normalized = (embedding_similarity - embedding_min) / (embedding_max - embedding_min) if embedding_max != embedding_min else 0.5\n",
    "\n",
    "    # Calculate hybrid score using normalized values\n",
    "    hybrid_score = (0.2 * rouge_normalized) + (0.2 * bleu_normalized) + (0.2 * f1_normalized) + (0.2 * bert_normalized) + (0.2 * embedding_normalized)\n",
    "    hybrid_scores.append(hybrid_score)\n",
    "\n",
    "    # Save the predicted answer and hybrid score in the DataFrame\n",
    "    predicted_answer = predicted_answers[index]\n",
    "    data.at[index, \"RAG 1\"] = predicted_answer\n",
    "\n",
    "    # Print each result\n",
    "    print(f\"[Processing question {index + 1}]:\\n\")\n",
    "    print(f\"ROUGE-L: {rouge}\")\n",
    "    print(f\"BLEU: {bleu}\")\n",
    "    print(f\"F1: {f1}\")\n",
    "    print(f\"BERT Score: {bert_score}\")\n",
    "    print(f\"Embedding Similarity: {embedding_similarity}\")\n",
    "    print(f\"Hybrid Score: {hybrid_score}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Save the DataFrame with predictions and hybrid scores to a new CSV file\n",
    "data.to_csv(\"../data/answers_rag1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluation Summary ===\n",
      "Average ROUGE-L: 0.1457\n",
      "Average BLEU: 2.25\n",
      "Average F1: 0.1738\n",
      "Average BERT Score: 0.8512\n",
      "Average Embedding Similarity: 0.4932\n",
      "Average Hybrid Score: 0.3591\n"
     ]
    }
   ],
   "source": [
    "# Save the evaluation metrics to a CSV file\n",
    "evaluation_metrics = {\n",
    "    \"ROUGE-L\": rouge_scores,\n",
    "    \"BLEU\": bleu_scores,\n",
    "    \"F1\": f1_scores,\n",
    "    \"BERT Score\": bert_scores,\n",
    "    \"Embedding Similarity\": embedding_similarities,\n",
    "    \"Hybrid Score\": hybrid_scores\n",
    "}\n",
    "evaluation_df = pd.DataFrame(evaluation_metrics)\n",
    "\n",
    "evaluation_df.to_csv(\"../data/rag1_evaluation_metrics.csv\", index=False)\n",
    "\n",
    "# Print evaluation summary\n",
    "print(\"=== Evaluation Summary ===\")\n",
    "print(f\"Average ROUGE-L: {sum(rouge_scores) / len(rouge_scores):.4f}\")\n",
    "print(f\"Average BLEU: {sum(bleu_scores) / len(bleu_scores):.2f}\")\n",
    "print(f\"Average F1: {sum(f1_scores) / len(f1_scores):.4f}\")\n",
    "print(f\"Average BERT Score: {sum(bert_scores) / len(bert_scores):.4f}\")\n",
    "print(f\"Average Embedding Similarity: {sum(embedding_similarities) / len(embedding_similarities):.4f}\")\n",
    "print(f\"Average Hybrid Score: {sum(hybrid_scores) / len(hybrid_scores):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
