{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import logging\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import google.generativeai as genai\n",
    "from google.oauth2 import service_account\n",
    "from pydantic import BaseModel, Field\n",
    "from rouge_score import rouge_scorer\n",
    "from collections import Counter\n",
    "import re\n",
    "import sacrebleu\n",
    "from collections import Counter\n",
    "from docx import Document as DocxDocument\n",
    "from tqdm import tqdm\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "from bert_score import score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"../data/question-answers.csv\"\n",
    "try:\n",
    "    # Load the CSV file with a specified encoding\n",
    "    data = pd.read_csv(csv_path, encoding=\"utf-8\")\n",
    "except UnicodeDecodeError:\n",
    "    # If utf-8 fails, try a different encoding\n",
    "    data = pd.read_csv(csv_path, encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\")\n",
    "                 )\n",
    "# Define text preprocessing function with lemmatization\n",
    "def preprocess_text(text):\n",
    "    # 1. Strip whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    # 2. Remove special characters and numbers (optional, depending on requirements)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "\n",
    "    # 3. Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # 4. Remove stopwords and apply lemmatization\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token.lower()) for token in tqdm(tokens, desc=\"Processing tokens\") if token.lower() not in stop_words]\n",
    "\n",
    "    # 5. Join the tokens back into a string\n",
    "    preprocessed_text = \" \".join(lemmatized_tokens)\n",
    "\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rouge(predicted, reference):\n",
    "    if predicted is None:\n",
    "        predicted = \"\"\n",
    "    if reference is None:\n",
    "        reference = \"\"\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rougeL\"], use_stemmer=True)\n",
    "    return scorer.score(reference, predicted)\n",
    "\n",
    "\n",
    "def evaluate_bleu(predicted, reference):\n",
    "    if predicted is None:\n",
    "        predicted = \"\"\n",
    "    if reference is None:\n",
    "        reference = \"\"\n",
    "    bleu = sacrebleu.corpus_bleu([predicted], [[reference]])\n",
    "    return bleu.score\n",
    "\n",
    "\n",
    "def evaluate_f1(predicted, reference):\n",
    "    if predicted is None:\n",
    "        predicted = \"\"\n",
    "    if reference is None:\n",
    "        reference = \"\"\n",
    "    predicted_tokens = nltk.word_tokenize(preprocess_text(predicted))\n",
    "    reference_tokens = nltk.word_tokenize(preprocess_text(reference))\n",
    "    predicted_counts = Counter(predicted_tokens)\n",
    "    reference_counts = Counter(reference_tokens)\n",
    "\n",
    "    common_tokens = sum((predicted_counts & reference_counts).values())\n",
    "\n",
    "    precision = common_tokens / sum(predicted_counts.values()) if predicted_counts else 0\n",
    "    recall = common_tokens / sum(reference_counts.values()) if reference_counts else 0\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "def evaluate_bert_score(predicted, reference):\n",
    "    if predicted is None:\n",
    "        predicted = \"\"\n",
    "    if reference is None:\n",
    "        reference = \"\"\n",
    "    P, R, F1 = score([predicted], [reference], lang=\"en\", verbose=False)\n",
    "    return F1.mean().item()\n",
    "\n",
    "\n",
    "# Initialize SentenceTransformer for embedding similarity\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "def evaluate_embedding_similarity(predicted, reference):\n",
    "    if predicted is None:\n",
    "        predicted = \"\"\n",
    "    if reference is None:\n",
    "        reference = \"\"\n",
    "    predicted_embedding = sentence_model.encode(predicted)\n",
    "    reference_embedding = sentence_model.encode(reference)\n",
    "    similarity = cosine_similarity([predicted_embedding], [reference_embedding])[0][0]\n",
    "    return similarity\n",
    "\n",
    "\n",
    "def evaluate_hybrid(predicted, reference):\n",
    "    rouge = evaluate_rouge(predicted, reference)\n",
    "    rouge_score = rouge[\"rougeL\"].fmeasure\n",
    "\n",
    "    bleu_score = evaluate_bleu(predicted, reference)\n",
    "    f1_score = evaluate_f1(predicted, reference)\n",
    "    bert_score = evaluate_bert_score(predicted, reference)\n",
    "    embedding_similarity = evaluate_embedding_similarity(predicted, reference)\n",
    "\n",
    "    # Define a weighted hybrid score\n",
    "    hybrid_score = (0.2 * rouge_score) + (0.2 * bleu_score) + (0.2 * f1_score) + (0.2 * bert_score) + (0.2 * embedding_similarity)\n",
    "    return hybrid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_number = input(\"Please enter the RAG number: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column for predicted answers\n",
    "data[f\"RAG {rag_number}\"] = \"\"\n",
    "\n",
    "# Initialize list to hold the predicted answers\n",
    "predicted_answers = []\n",
    "\n",
    "# Initialize lists to hold metric scores\n",
    "rouge_scores, bleu_scores, f1_scores, bert_scores, embedding_similarities, hybrid_scores = [], [], [], [], [], []\n",
    "\n",
    "# Initialize min and max values for each metric\n",
    "rouge_min, rouge_max = float(\"inf\"), float(\"-inf\")\n",
    "bleu_min, bleu_max = float(\"inf\"), float(\"-inf\")\n",
    "f1_min, f1_max = float(\"inf\"), float(\"-inf\")\n",
    "bert_min, bert_max = float(\"inf\"), float(\"-inf\")\n",
    "embedding_min, embedding_max = float(\"inf\"), float(\"-inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First pass to collect metric values and determine min and max for normalization\n",
    "for index, row in tqdm(data.iterrows(), total=len(data), desc=\"Processing questions\", leave=False):\n",
    "    question = row[\"Question\"]\n",
    "    reference_answer = row[\"Answer\"]\n",
    "    predicted_answer = answer_query(question)\n",
    "    predicted_answers.append(predicted_answer)\n",
    "\n",
    "    # Evaluate the metrics\n",
    "    rouge = evaluate_rouge(predicted_answer, reference_answer)\n",
    "    bleu = evaluate_bleu(predicted_answer, reference_answer)\n",
    "    f1 = evaluate_f1(predicted_answer, reference_answer)\n",
    "    bert_score = evaluate_bert_score(predicted_answer, reference_answer)\n",
    "    embedding_similarity = evaluate_embedding_similarity(predicted_answer, reference_answer)\n",
    "\n",
    "    # Update min and max values for each metric\n",
    "    rouge_scores.append(rouge[\"rougeL\"].fmeasure)\n",
    "    bleu_scores.append(bleu)\n",
    "    f1_scores.append(f1)\n",
    "    bert_scores.append(bert_score)\n",
    "    embedding_similarities.append(embedding_similarity)\n",
    "\n",
    "    rouge_min, rouge_max = min(rouge_min, rouge[\"rougeL\"].fmeasure), max(rouge_max, rouge[\"rougeL\"].fmeasure)\n",
    "    bleu_min, bleu_max = min(bleu_min, bleu), max(bleu_max, bleu)\n",
    "    f1_min, f1_max = min(f1_min, f1), max(f1_max, f1)\n",
    "    bert_min, bert_max = min(bert_min, bert_score), max(bert_max, bert_score)\n",
    "    embedding_min, embedding_max = min(embedding_min, embedding_similarity), max(embedding_max, embedding_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second pass to normalize the metrics and calculate the hybrid score\n",
    "for index in tqdm(range(len(data)), desc=\"Calculating hybrid scores\"):\n",
    "    rouge = rouge_scores[index]\n",
    "    bleu = bleu_scores[index]\n",
    "    f1 = f1_scores[index]\n",
    "    bert_score = bert_scores[index]\n",
    "    embedding_similarity = embedding_similarities[index]\n",
    "\n",
    "    # Normalize each metric\n",
    "    rouge_normalized = (rouge - rouge_min) / (rouge_max - rouge_min) if rouge_max != rouge_min else 0.5\n",
    "    bleu_normalized = (bleu - bleu_min) / (bleu_max - bleu_min) if bleu_max != bleu_min else 0.5\n",
    "    f1_normalized = (f1 - f1_min) / (f1_max - f1_min) if f1_max != f1_min else 0.5\n",
    "    bert_normalized = (bert_score - bert_min) / (bert_max - bert_min) if bert_max != bert_min else 0.5\n",
    "    embedding_normalized = (embedding_similarity - embedding_min) / (embedding_max - embedding_min) if embedding_max != embedding_min else 0.5\n",
    "\n",
    "    # Calculate hybrid score using normalized values\n",
    "    hybrid_score = (0.2 * rouge_normalized) + (0.2 * bleu_normalized) + (0.2 * f1_normalized) + (0.2 * bert_normalized) + (0.2 * embedding_normalized)\n",
    "    hybrid_scores.append(hybrid_score)\n",
    "\n",
    "    # Save the predicted answer and hybrid score in the DataFrame\n",
    "    predicted_answer = predicted_answers[index]\n",
    "    data.at[index, \"RAG 1\"] = predicted_answer\n",
    "\n",
    "    # Print each result\n",
    "    print(f\"[Processing question {index + 1}]:\\n\")\n",
    "    print(f\"ROUGE-L: {rouge}\")\n",
    "    print(f\"BLEU: {bleu}\")\n",
    "    print(f\"F1: {f1}\")\n",
    "    print(f\"BERT Score: {bert_score}\")\n",
    "    print(f\"Embedding Similarity: {embedding_similarity}\")\n",
    "    print(f\"Hybrid Score: {hybrid_score}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Save the DataFrame with predictions and hybrid scores to a new CSV file\n",
    "data.to_csv(f\"../data/answers_rag{rag_number}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the evaluation metrics to a CSV file\n",
    "evaluation_metrics = {\n",
    "    \"ROUGE-L\": rouge_scores,\n",
    "    \"BLEU\": bleu_scores,\n",
    "    \"F1\": f1_scores,\n",
    "    \"BERT Score\": bert_scores,\n",
    "    \"Embedding Similarity\": embedding_similarities,\n",
    "    \"Hybrid Score\": hybrid_scores\n",
    "}\n",
    "evaluation_df = pd.DataFrame(evaluation_metrics)\n",
    "\n",
    "evaluation_df.to_csv(f\"../data/rag{rag_number}_evaluation_metrics.csv\", index=False)\n",
    "\n",
    "# Print evaluation summary\n",
    "print(\"=== Evaluation Summary ===\")\n",
    "print(f\"Average ROUGE-L: {sum(rouge_scores) / len(rouge_scores):.4f}\")\n",
    "print(f\"Average BLEU: {sum(bleu_scores) / len(bleu_scores):.2f}\")\n",
    "print(f\"Average F1: {sum(f1_scores) / len(f1_scores):.4f}\")\n",
    "print(f\"Average BERT Score: {sum(bert_scores) / len(bert_scores):.4f}\")\n",
    "print(f\"Average Embedding Similarity: {sum(embedding_similarities) / len(embedding_similarities):.4f}\")\n",
    "print(f\"Average Hybrid Score: {sum(hybrid_scores) / len(hybrid_scores):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
